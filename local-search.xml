<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Unsloth框架介绍</title>
    <link href="/2025/06/25/Unsloth%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/"/>
    <url>/2025/06/25/Unsloth%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="Unsloth框架介绍"><a href="#Unsloth框架介绍" class="headerlink" title="Unsloth框架介绍"></a>Unsloth框架介绍</h1><p>Unsloth 是为大语言模型<b>高效地</b>进行<u>监督微调</u>和<u>强化学习</u>的<u>开源框架</u>。高效性体现在对主流开源大模型（Llama, DeepSeek, Qwen, Gemma 等）的<u>训练、推理、评估和权重保存</u>都有 2 倍<b>快</b>的速度，并且显存占用 VRAM <b>更小</b>。Unsloth 适合于<u>本地训练</u>以及<u>云服务器平台</u>（Google Colab, Kaggle 和阿里云平台）。Unsloth 将从<u>模型训练</u>到<u>模型保存</u>的所有过程都<b>流水线化</b>。同时，Unsloth 解决了主流模型之间的关键 bug ，提高了训练模型的<u>精确性、稳定性和提示词的把控性</u>。对用户而言，Unsloth 具有高度客制化性，用户可以操作聊天模板和数据集格式。且内置多模态模型使用和强化学习优化的案例。</p>]]></content>
    
    
    
    <tags>
      
      <tag>框架</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态大模型评测体系MMBench学习与使用</title>
    <link href="/2025/06/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E4%BD%93%E7%B3%BBMMBench%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2025/06/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E4%BD%93%E7%B3%BBMMBench%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="多模态大模型评测体系-MMBench-学习与使用"><a href="#多模态大模型评测体系-MMBench-学习与使用" class="headerlink" title="多模态大模型评测体系 MMBench 学习与使用"></a>多模态大模型评测体系 MMBench 学习与使用</h1><h2 id="MMBench-介绍"><a href="#MMBench-介绍" class="headerlink" title="MMBench 介绍"></a>MMBench 介绍</h2><p>官网：<a href="https://mmbench.opencompass.org.cn/leaderboard">此处进入</a>。</p><h3 id="MMbench-如何计算评估分数？"><a href="#MMbench-如何计算评估分数？" class="headerlink" title="MMbench 如何计算评估分数？"></a>MMbench 如何计算评估分数？</h3><ol><li>MMBench 将推理 (reasoning) 和感知 (perception) 能力细分为 6 个能力维度.</li><li>推理：逻辑推理 (Logic Reasoning, LR) 、属性推理 (Attribute Reasoning, AR) 、关系推理 (Relation Reasoning, RR) .</li><li>感知：细粒度单感知实例 (Fine-Grained Perception-Single Instance, FP-S) 、细粒度跨感知实例 (Fine-Grained Perception-Cross Instance, FP-C) 、粗感知 (Coarse Perception, CP) .</li><li>平均分数来自<u>所有 3 级能力的平均分数</u>，2 级能力维度分数来自<u>该维度内所有 3 级能力分数的平均值</u>。</li><li>在开发&#x2F;测试集之间切换以查看不同模型在不同集上的得分。测试集的基本事实不公开。</li></ol><h3 id="MMBench-的评估流程？"><a href="#MMBench-的评估流程？" class="headerlink" title="MMBench 的评估流程？"></a>MMBench 的评估流程？</h3><p><img src="/process.png" alt="MMBench 评估流程"></p><p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：<strong>配置</strong> -&gt; <strong>推理</strong> -&gt; <strong>评估</strong> -&gt; <strong>可视化</strong>。</p><p><strong>配置</strong>：这是整个工作流的起点。您需要配置整个评估过程，选择要评估的模型和数据集。此外，还可以选择评估策略、计算后端等，并定义显示结果的方式。</p><p><strong>推理与评估</strong>：在这个阶段，OpenCompass 将会开始对模型和数据集进行并行推理和评估。<strong>推理</strong>阶段主要是让模型从数据集产生输出，而<strong>评估</strong>阶段则是衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的 “任务” 以提高效率，但请注意，如果计算资源有限，这种策略可能会使评测变得更慢。如果需要了解该问题及解决方案，可以参考 <a href="https://opencompass.readthedocs.io/zh-cn/latest/get_started/faq.html#id6">FAQ: 效率</a> 。</p><p><strong>可视化</strong>：评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。你也可以激活飞书状态上报功能，此后可以在飞书客户端中及时获得评测状态报告。</p><p>数据集配置通常有两种类型：<code>ppl</code> 和 <code>gen</code>，分别指示使用的评估方法。其中 <code>ppl</code> 表示辨别性评估，<code>gen</code> 表示生成性评估。对话模型仅使用 <code>gen</code> 生成式评估。</p><p>由于 OpenCompass 默认并行启动评估过程，我们可以在第一次运行时以 <code>--debug</code> 模式启动评估，并检查是否存在问题。在 <code>--debug</code> 模式下，任务将按顺序执行，并实时打印输出。</p><h3 id="ppl-和-gen-有什么区别和联系？"><a href="#ppl-和-gen-有什么区别和联系？" class="headerlink" title="ppl 和 gen 有什么区别和联系？"></a>ppl 和 gen 有什么区别和联系？</h3><p><code>ppl</code> 是困惑度 (perplexity) 的缩写，是一种评价模型进行语言建模能力的指标。在 OpenCompass 的语境下，它一般指一种选择题的做法：给定一个上下文，模型需要从多个备选项中选择一个最合适的。此时，我们会将 n 个选项拼接上上下文后，形成 n 个序列，然后计算模型对这 n 个序列的 perplexity，我们认为其中 perplexity 最低的序列所对应的选项即为模型在这道题上面的推理结果，该种评测方法的后处理简单直接、确定性高。</p><p><code>gen</code> 是生成 (generate) 的缩写。在 OpenCompass 的语境下，它指的是在给定上下文的情况下，模型往后续写的结果就是这道题目上的推理结果。一般来说，续写得到的字符串需要结合上比较重的后处理过程，才能进行可靠的答案提取，从而完成评测。</p><p>从使用上来说，基座模型的单项选择题和部分具有选择题性质的题目会使用 <code>ppl</code>，基座模型的不定项选择和非选择题都会使用 <code>gen</code>。而对话模型的所有题目都会使用 <code>gen</code>，因为许多商用 API 模型不会暴露 <code>ppl</code> 的接口。但也存在例外情况，例如我们希望基座模型输出解题思路过程时 (例如 Let’s think step by step)，我们同样会使用 <code>gen</code>。</p><p>与 <code>ppl</code> 高度类似地，条件对数概率 <code>clp</code> (conditional log probability) 是在给定上下文的情况下，计算下一个 token 的概率。它也仅适用于选择题，考察概率的范围仅限于备选项标号所对应的 token，取其中概率最高的 token 所对应的选项为模型的推理结果。与 ppl 相比，<code>clp</code> 的计算更加高效，仅需要推理一次，而 ppl 需要推理 n 次，但坏处是，<code>clp</code> 受制于 tokenizer，在例如选项前后有无空格符号时，tokenizer 编码的结果会有变化，导致测试结果不可靠。因此 OpenCompass 中很少使用 <code>clp</code> 。</p><h3 id="OpenCompass-如何控制-few-shot-评测的-shot-数目？"><a href="#OpenCompass-如何控制-few-shot-评测的-shot-数目？" class="headerlink" title="OpenCompass 如何控制 few shot 评测的 shot 数目？"></a>OpenCompass 如何控制 few shot 评测的 shot 数目？</h3><p>在数据集配置文件中，有一个 <code>retriever</code> 的字段，该字段表示如何召回数据集中的样本作为上下文样例，其中最常用的是 <code>FixKRetriever</code> 表示固定使用某 k 个样本，因此即为 k-shot。另外还有 <code>ZeroRetriever</code> 表示不使用任何样本，这在大多数情况下意味着 0-shot。</p><h3 id="OpenCompass-task-的默认划分逻辑是什么样的？"><a href="#OpenCompass-task-的默认划分逻辑是什么样的？" class="headerlink" title="OpenCompass task 的默认划分逻辑是什么样的？"></a>OpenCompass task 的默认划分逻辑是什么样的？</h3><p>OpenCompass 默认使用 num_worker_partitioner。OpenCompass 的评测从本质上来说就是有一系列的模型和一系列的数据集，然后两两组合，用每个模型去跑每个数据集。对于同一个模型，OpenCompass 会将其拆分为 <code>--max-num-workers</code> (或 config 中的 <code>infer.runner.max_num_workers</code>) 个 task，为了保证每个 task 的运行耗时均匀，每个 task 均会所有数据集的一部分。</p><h2 id="MMBench-使用"><a href="#MMBench-使用" class="headerlink" title="MMBench 使用"></a>MMBench 使用</h2><p>OpenCompass 要求 <code>pytorch&gt;=1.13</code> 。</p><p>完整安装（支持更多数据集）<code>pip install &quot;opencompass[full]&quot;</code> </p><p>API 测试（例如 OpenAI &#x2F; Qwen）<code>pip install &quot;opencompass[api]&quot;</code></p><p>如果推理后端是 vllm ，那么用以下指令验证成功 <code>vllm serve facebook/opt-125m</code></p><p>针对 HumanEvalX &#x2F; HumanEval+ &#x2F; MBPP+ 需要手动克隆 git 仓库进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recurse-submodules git@github.com:open-compass/human-eval.git<br><span class="hljs-built_in">cd</span> human-eval<br>pip install -e .<br>pip install -e evalplus<br></code></pre></td></tr></table></figure><p>OpenCompass 支持的数据集主要包括三个部分：</p><ol><li><p>Huggingface 数据集： <a href="https://huggingface.co/datasets">Huggingface Dataset</a> 提供了大量的数据集，这部分数据集运行时会<strong>自动下载</strong>。</p></li><li><p>ModelScope 数据集：<a href="https://modelscope.cn/organization/opencompass">ModelScope OpenCompass Dataset</a> 支持从 ModelScope 自动下载数据集。</p><p>要启用此功能，请设置环境变量：<code>export DATASET_SOURCE=ModelScope</code>，可用的数据集包括（来源于 <code>OpenCompassData-core.zip</code>）：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">humaneval, triviaqa, commonsenseqa, tydiqa, strategyqa, cmmlu, lambada, piqa, ceval, math, LCSTS, Xsum, winogrande, openbookqa, AGIEval, gsm8k, nq, race, siqa, mbpp, mmlu, hellaswag, ARC, <span class="hljs-keyword">BBH, </span>xstory_cloze, summedits, GAOKAO-<span class="hljs-keyword">BENCH, </span>OCNLI, cmnli<br></code></pre></td></tr></table></figure></li><li><p>自建以及第三方数据集：OpenCompass 还提供了一些第三方数据集及自建<strong>中文</strong>数据集。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>评估</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
