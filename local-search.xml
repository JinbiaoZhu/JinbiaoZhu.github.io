<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记_第1篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第一代-Qwen-VL-2023-08-2023-10"><a href="#第一代-Qwen-VL-2023-08-2023-10" class="headerlink" title="第一代 Qwen-VL 2023.08-2023.10"></a>第一代 Qwen-VL 2023.08-2023.10</h1><p>当时大多数的 LVLMs 都是以<b>粗粒度</b>的方式感知图像，缺乏<u>图像细粒度感知</u>的能力（包括<b>目标定位</b>和<b>文本读取</b>等）。基于当时的问题，Qwen 团队引入了一个<u>新的视觉编码器</u>和<u>位置感知适配器</u>，并且设计了一个三阶段训练的流程用于优化 Qwen-VL 模型。Qwen-VL 的特点：性能领先、支持多语言、<strong>支持任意交错的 “图像-文本” 数据</strong>、<strong>细粒度的视觉理解</strong>（例如 OCR）。Qwen-VL 相较于之前的<b>图文多模态大模型</b>多了一个功能：视觉定位，就是可以<b>给出一个框</b>将你想要的地方框出来。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/2025/06/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E4%BD%93%E7%B3%BBMMBench%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/wenvl_01.png"></p><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p>Qwen-VL 整个模型参数大致在 1.9B + 0.08B + 7.7B &#x3D; 9.6B 的参数数量。</p><ol><li><p>语言模型：Qwen-7B 大语言模型；</p></li><li><p>视觉编码器：ViT 的架构，参数量在 1.9B ，并且从<a href="https://github.com/mlfoundations/open_clip">开源项目 openclip 的 ViT-bigG</a> 权重开始初始化，训练和推理的过程中图像会被调整到特定的分辨率，也就是拆成 14x14 像素的 patch 块；</p></li><li><p>（位置级）视觉语言适配器：一个<u>随机权重初始化</u>的<u>单层交叉注意力模块</u>组成，参数量在 0.08B 。</p><p>该模块使用一组<b>可训练的向量（意思就是在训练中张量数值会改变，且梯度会流向这个向量）</b>作为 query 向量，将<b>视觉编码器的特征</b>作为 key 进行交叉注意力操作，将图像特征压缩到 256 长度的序列。并且将 2D 绝对位置编码用在交叉注意力机制中，以减轻压缩过程中的位置细节丢失。</p></li></ol><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：<code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code> 标记图像的开始和结束。图片通过<u>视觉编码器</u>和<u>（位置级）视觉语言适配器</u>模块，得到一个定长的特征序列。为了和文字输入区别，图片特征前后分别加上 <code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code>。</p><p>边界框输出：将边界框的值归一化在 <code>[0,1000)</code> 之间，并转换成特定的字符串格式 <code>&quot;(X_top_left, Y_top_left), (X_bottom_right, Y_bottom_right)&quot;</code> ，<code>&lt;box&gt;</code> 和 <code>&lt;/box&gt;</code> 分别添加在边界框字符串的开头和结尾。</p><p>内容输出：<code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code> 标记边界框所引用的内容。</p><blockquote><p>例如，某个任务的提示词：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">&lt;<span class="hljs-selector-tag">img</span>&gt;coyo700m/<span class="hljs-number">1</span><span class="hljs-selector-class">.jpg</span>&lt;/<span class="hljs-selector-tag">img</span>&gt;Generate the <span class="hljs-selector-tag">caption</span> in English with grounding:<br></code></pre></td></tr></table></figure><p>Qwen-VL 的回答如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Beautiful</span> shot of &lt;ref&gt;bees&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">661</span>,<span class="hljs-number">612</span>),(<span class="hljs-number">833</span>,<span class="hljs-number">812</span>)&lt;/box&gt;&lt;box&gt;(<span class="hljs-number">120</span>,<span class="hljs-number">555</span>),(<span class="hljs-number">265</span>,<span class="hljs-number">770</span>)&lt;/box&gt; gathering nectars from &lt;ref&gt;an apricot flower&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">224</span>,<span class="hljs-number">13</span>),(<span class="hljs-number">399</span>,<span class="hljs-number">313</span>) &lt;/box&gt;&lt;eos&gt;<br></code></pre></td></tr></table></figure></blockquote><p>模型处理视觉信息的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 确保只在第一次 forward 时处理视觉信息（past_key_values is None 表示不是缓存推理时）</span><br><span class="hljs-comment"># 图像 token 是以特殊的 image_start_id 开始，检测是否存在</span><br><span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> torch.<span class="hljs-built_in">any</span>(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>]):<br>    <br>    <span class="hljs-comment"># 找到图像 token 的边界位置</span><br>        bos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>])<br>        eos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 保证每个起始标记都在一个样本内部结束</span><br>        <span class="hljs-keyword">assert</span> (bos_pos[<span class="hljs-number">0</span>] == eos_pos[<span class="hljs-number">0</span>]).<span class="hljs-built_in">all</span>()<br>        <br>        <span class="hljs-comment"># 构建 img_pos：(batch_idx, start_idx, end_idx)</span><br>        img_pos = torch.stack((bos_pos[<span class="hljs-number">0</span>], bos_pos[<span class="hljs-number">1</span>], eos_pos[<span class="hljs-number">1</span>]), dim=<span class="hljs-number">1</span>)<br>        images = []<br>        <span class="hljs-keyword">for</span> i, a, b <span class="hljs-keyword">in</span> img_pos:<br>            <br>            <span class="hljs-comment"># 截取 patch token（跳过起始和终止标志）</span><br>            image = input_ids[i][a + <span class="hljs-number">1</span> : b - <span class="hljs-number">1</span>].tolist()<br>            <span class="hljs-comment"># 截取图片结束 token</span><br>            image = image[ : image.index(<span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">2</span>)]<br>            <span class="hljs-comment"># 解码为 utf-8 图像数据（说明 image token 实际是原始图片字节的编码）</span><br>            images.append(<span class="hljs-built_in">bytes</span>(image).decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>            <br>            <span class="hljs-comment"># 调用视觉编码器</span><br>            images = <span class="hljs-variable language_">self</span>.visual.encode(images)<br>            <span class="hljs-keyword">assert</span> images.shape[<span class="hljs-number">0</span>] == <span class="hljs-built_in">len</span>(images)<br>            <br>            fake_images = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">if</span> fake_images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                hidden_states = hidden_states + images.mean()*<span class="hljs-number">0</span><br>            <br><span class="hljs-comment"># 将图像嵌入写入对应位置 a+1 : b 的 hidden state（对应 patch tokens）</span><br>            <span class="hljs-keyword">elif</span> images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">for</span> idx, (i, a, b) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(img_pos):<br>                    hidden_states[i][a + <span class="hljs-number">1</span> : b] = images[idx]<br></code></pre></td></tr></table></figure><blockquote><p>Q：<code>image = image[ : image.index(self.config.visual[&#39;image_start_id&#39;] + 2)]</code> 这段代码的作用？</p><p>A：因为模型在输入图片时，有时会预留更多 token 空间来填充图像信息，例如 padding 和 filter ，这就导致图像本身可能变长。这些填充的 token 确实是图像的一部分，但是输入到视觉编码器中产生干扰，因此需要额外再加一行代码对这些填充 token 做进一步过滤。</p></blockquote><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：预训练过程"><a href="#第一阶段：预训练过程" class="headerlink" title="第一阶段：预训练过程"></a>第一阶段：预训练过程</h3><p>使用互联网网页抓取的 ”图像—文本“ 对，50 亿条数据清洗后剩下 14 亿数据，其中 77.3% 为英文数据，22.7% 为中文数据。<b>这一阶段冻结语言模型</b>，训练视觉编码器和视觉语言适配器，输入图像调整为 224x224 的分辨率（按照每 14 像素分割后得到 16x16&#x3D;256 个 patch），batch size 为 30720 ，训练 50000 步，使用 15 亿数据。</p><h3 id="第二阶段：多任务预训练"><a href="#第二阶段：多任务预训练" class="headerlink" title="第二阶段：多任务预训练"></a>第二阶段：多任务预训练</h3><p>加入了高质量、细粒度的图像和文本数据，使用了更大的分辨率和交错的 ”图像—文本“ 数据。在 7 个任务上对 Qwen-VL 进行训练。将视觉编码器的分辨率从 224x224 增加到 448x448，以减少图像下采样造成的信息损失。<b>这一过程没有冻结任何模块。</b></p><h3 id="第三阶段：监督微调"><a href="#第三阶段：监督微调" class="headerlink" title="第三阶段：监督微调"></a>第三阶段：监督微调</h3><p>数据<u>来自 LLM 生成</u>的图像标注或对话数据，这些数据通常只处理<b>单图像对话和推理</b>，且仅限于图像内容理解。</p><p>通过<u>手动标注、模型生成和策略组合</u>，构建了一个额外的对话数据集，以将<b>定位和多图像理解能力</b>融入 Qwen-VL 模型中。在训练过程中混合了多模态和<u>纯文本对话数据</u>，以确保模型的对话能力具有普遍性。</p><p>指令微调数据量达到 35 万条。<b>这一过程冻结视觉编码器。</b></p><h2 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h2><h2 id="模型代码应用"><a href="#模型代码应用" class="headerlink" title="模型代码应用"></a>模型代码应用</h2><h3 id="图片和文本的加载"><a href="#图片和文本的加载" class="headerlink" title="图片和文本的加载"></a>图片和文本的加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">query = tokenizer.from_list_format([<br>    &#123;<span class="hljs-string">&#x27;image&#x27;</span>: <span class="hljs-string">&#x27;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#x27;</span>&#125;,<br>    &#123;<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;这是什么&#x27;</span>&#125;,<br>])<br></code></pre></td></tr></table></figure><h3 id="图像到字符串的转换"><a href="#图像到字符串的转换" class="headerlink" title="图像到字符串的转换"></a>图像到字符串的转换</h3><p>Qwen-VL 将图片都处理成：“Picture 1”、“Picture 2”、“Picture 3” 等<u>字符串</u>格式，并添加上<u>图片的开始和结束 token</u> ，文本直接拼接，box 的 <u>ref 添加上开始结束符</u>拼接，box <u>坐标从数字整理成字符串格式</u>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_list_format</span>(<span class="hljs-params">self, list_format: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>]</span>):<br>    text = <span class="hljs-string">&#x27;&#x27;</span><br>    num_images = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> list_format:  <span class="hljs-comment"># 每个 ele 都是字典</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 图片处理成这样的字符串，再加上图像自身、开始和结束的 tokens</span><br>            num_images += <span class="hljs-number">1</span><br>            text += <span class="hljs-string">f&#x27;Picture <span class="hljs-subst">&#123;num_images&#125;</span>: &#x27;</span><br>            text += <span class="hljs-variable language_">self</span>.image_start_tag + ele[<span class="hljs-string">&#x27;image&#x27;</span>] + <span class="hljs-variable language_">self</span>.image_end_tag<br>            text += <span class="hljs-string">&#x27;\n&#x27;</span><br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是文本，直接添加文本</span><br>            text += ele[<span class="hljs-string">&#x27;text&#x27;</span>]<br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;box&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是定位框，先考虑有没有参考对象</span><br>            <span class="hljs-comment"># 如果有的话，先添加参考对象自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-comment"># 没有的话，添加定位框自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ref&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>                text += <span class="hljs-variable language_">self</span>.ref_start_tag + ele[<span class="hljs-string">&#x27;ref&#x27;</span>] + <span class="hljs-variable language_">self</span>.ref_end_tag<br>            <span class="hljs-keyword">for</span> box <span class="hljs-keyword">in</span> ele[<span class="hljs-string">&#x27;box&#x27;</span>]:<br>                text += <span class="hljs-variable language_">self</span>.box_start_tag + <span class="hljs-string">&#x27;(%d,%d),(%d,%d)&#x27;</span> % (box[<span class="hljs-number">0</span>], box[<span class="hljs-number">1</span>], box[<span class="hljs-number">2</span>], box[<span class="hljs-number">3</span>]) + <span class="hljs-variable language_">self</span>.box_end_tag<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Unsupport element: &quot;</span> + <span class="hljs-built_in">str</span>(ele))<br>    <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure><h3 id="图像的编码"><a href="#图像的编码" class="headerlink" title="图像的编码"></a>图像的编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, image_paths: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>):<br>        images = []<br>        <span class="hljs-keyword">for</span> image_path <span class="hljs-keyword">in</span> image_paths:<br>            <span class="hljs-keyword">if</span> image_path.startswith(<span class="hljs-string">&quot;http://&quot;</span>) <span class="hljs-keyword">or</span> image_path.startswith(<span class="hljs-string">&quot;https://&quot;</span>):<br>                image = Image.<span class="hljs-built_in">open</span>(requests.get(image_path, stream=<span class="hljs-literal">True</span>).raw)<br>            <span class="hljs-keyword">else</span>:<br>                image = Image.<span class="hljs-built_in">open</span>(image_path)<br>            image = image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>            images.append(<span class="hljs-variable language_">self</span>.image_transform(image))<br>        images = torch.stack(images, dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>(images)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: torch.Tensor</span>):<br>        x = x.to(<br>            dtype=<span class="hljs-variable language_">self</span>.transformer.get_cast_dtype(),<br>            device=<span class="hljs-variable language_">self</span>.transformer.get_cast_device(),<br>        )<br>        <span class="hljs-comment"># to patches</span><br>        x = <span class="hljs-variable language_">self</span>.conv1(x)  <span class="hljs-comment"># shape = [*, width, grid, grid]</span><br>        x = x.reshape(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, width, grid ** 2]</span><br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, grid ** 2, width]</span><br><br>        x = x + get_abs_pos(<span class="hljs-variable language_">self</span>.positional_embedding, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.ln_pre(x)<br><br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># NLD -&gt; LND</span><br>        x = <span class="hljs-variable language_">self</span>.transformer(x)<br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># LND -&gt; NLD</span><br><br>        x = <span class="hljs-variable language_">self</span>.attn_pool(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_post(x)<br>        x = x @ <span class="hljs-variable language_">self</span>.proj<br></code></pre></td></tr></table></figure><p>图片是经过 resize 和归一化后输入 ViT 进行编码，ViT 编码后经过交叉注意力机制、归一化然后投影到 embedding 维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.attn_pool = Resampler(<br>            grid_size=<span class="hljs-built_in">int</span>(math.sqrt(n_queries)),<br>            embed_dim=output_dim,<br>            num_heads=output_dim // <span class="hljs-number">128</span>,<br>            kv_dim=width,<br>            norm_layer=norm_layer,<br>        )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, attn_mask=<span class="hljs-literal">None</span></span>):<br><br>        pos_embed = get_abs_pos(<span class="hljs-variable language_">self</span>.pos_embed, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.kv_proj(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_kv(x).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br><br>        N = x.shape[<span class="hljs-number">1</span>]<br>        q = <span class="hljs-variable language_">self</span>.ln_q(<span class="hljs-variable language_">self</span>.query)<br>        out = <span class="hljs-variable language_">self</span>.attn(<br>            <span class="hljs-variable language_">self</span>._repeat(q, N) + <span class="hljs-variable language_">self</span>.pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x + pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x,<br>            attn_mask=attn_mask)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> out.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Unsloth框架介绍</title>
    <link href="/2025/06/25/Unsloth%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/"/>
    <url>/2025/06/25/Unsloth%E6%A1%86%E6%9E%B6%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h1 id="Unsloth框架介绍"><a href="#Unsloth框架介绍" class="headerlink" title="Unsloth框架介绍"></a>Unsloth框架介绍</h1><p>Unsloth 是为大语言模型<b>高效地</b>进行<u>监督微调</u>和<u>强化学习</u>的<u>开源框架</u>。高效性体现在对主流开源大模型（Llama, DeepSeek, Qwen, Gemma 等）的<u>训练、推理、评估和权重保存</u>都有 2 倍<b>快</b>的速度，并且显存占用 VRAM <b>更小</b>。Unsloth 适合于<u>本地训练</u>以及<u>云服务器平台</u>（Google Colab, Kaggle 和阿里云平台）。Unsloth 将从<u>模型训练</u>到<u>模型保存</u>的所有过程都<b>流水线化</b>。同时，Unsloth 解决了主流模型之间的关键 bug ，提高了训练模型的<u>精确性、稳定性和提示词的把控性</u>。对用户而言，Unsloth 具有高度客制化性，用户可以操作聊天模板和数据集格式。且内置多模态模型使用和强化学习优化的案例。</p>]]></content>
    
    
    
    <tags>
      
      <tag>框架</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>多模态大模型评测体系MMBench学习与使用</title>
    <link href="/2025/06/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E4%BD%93%E7%B3%BBMMBench%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2025/06/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E4%BD%93%E7%B3%BBMMBench%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h1 id="多模态大模型评测体系-MMBench-学习与使用"><a href="#多模态大模型评测体系-MMBench-学习与使用" class="headerlink" title="多模态大模型评测体系 MMBench 学习与使用"></a>多模态大模型评测体系 MMBench 学习与使用</h1><h2 id="MMBench-介绍"><a href="#MMBench-介绍" class="headerlink" title="MMBench 介绍"></a>MMBench 介绍</h2><p>官网：<a href="https://mmbench.opencompass.org.cn/leaderboard">此处进入</a>。</p><h3 id="MMbench-如何计算评估分数？"><a href="#MMbench-如何计算评估分数？" class="headerlink" title="MMbench 如何计算评估分数？"></a>MMbench 如何计算评估分数？</h3><ol><li>MMBench 将推理 (reasoning) 和感知 (perception) 能力细分为 6 个能力维度.</li><li>推理：逻辑推理 (Logic Reasoning, LR) 、属性推理 (Attribute Reasoning, AR) 、关系推理 (Relation Reasoning, RR) .</li><li>感知：细粒度单感知实例 (Fine-Grained Perception-Single Instance, FP-S) 、细粒度跨感知实例 (Fine-Grained Perception-Cross Instance, FP-C) 、粗感知 (Coarse Perception, CP) .</li><li>平均分数来自<u>所有 3 级能力的平均分数</u>，2 级能力维度分数来自<u>该维度内所有 3 级能力分数的平均值</u>。</li><li>在开发&#x2F;测试集之间切换以查看不同模型在不同集上的得分。测试集的基本事实不公开。</li></ol><h3 id="MMBench-的评估流程？"><a href="#MMBench-的评估流程？" class="headerlink" title="MMBench 的评估流程？"></a>MMBench 的评估流程？</h3><p><img src="/2025/06/25/%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E6%A8%A1%E5%9E%8B%E8%AF%84%E6%B5%8B%E4%BD%93%E7%B3%BBMMBench%E5%AD%A6%E4%B9%A0%E4%B8%8E%E4%BD%BF%E7%94%A8/process.png"></p><p>在 OpenCompass 中评估一个模型通常包括以下几个阶段：<strong>配置</strong> -&gt; <strong>推理</strong> -&gt; <strong>评估</strong> -&gt; <strong>可视化</strong>。</p><p><strong>配置</strong>：这是整个工作流的起点。您需要配置整个评估过程，选择要评估的模型和数据集。此外，还可以选择评估策略、计算后端等，并定义显示结果的方式。</p><p><strong>推理与评估</strong>：在这个阶段，OpenCompass 将会开始对模型和数据集进行并行推理和评估。<strong>推理</strong>阶段主要是让模型从数据集产生输出，而<strong>评估</strong>阶段则是衡量这些输出与标准答案的匹配程度。这两个过程会被拆分为多个同时运行的 “任务” 以提高效率，但请注意，如果计算资源有限，这种策略可能会使评测变得更慢。如果需要了解该问题及解决方案，可以参考 <a href="https://opencompass.readthedocs.io/zh-cn/latest/get_started/faq.html#id6">FAQ: 效率</a> 。</p><p><strong>可视化</strong>：评估完成后，OpenCompass 将结果整理成易读的表格，并将其保存为 CSV 和 TXT 文件。你也可以激活飞书状态上报功能，此后可以在飞书客户端中及时获得评测状态报告。</p><p>数据集配置通常有两种类型：<code>ppl</code> 和 <code>gen</code>，分别指示使用的评估方法。其中 <code>ppl</code> 表示辨别性评估，<code>gen</code> 表示生成性评估。对话模型仅使用 <code>gen</code> 生成式评估。</p><p>由于 OpenCompass 默认并行启动评估过程，我们可以在第一次运行时以 <code>--debug</code> 模式启动评估，并检查是否存在问题。在 <code>--debug</code> 模式下，任务将按顺序执行，并实时打印输出。</p><h3 id="ppl-和-gen-有什么区别和联系？"><a href="#ppl-和-gen-有什么区别和联系？" class="headerlink" title="ppl 和 gen 有什么区别和联系？"></a>ppl 和 gen 有什么区别和联系？</h3><p><code>ppl</code> 是困惑度 (perplexity) 的缩写，是一种评价模型进行语言建模能力的指标。在 OpenCompass 的语境下，它一般指一种选择题的做法：给定一个上下文，模型需要从多个备选项中选择一个最合适的。此时，我们会将 n 个选项拼接上上下文后，形成 n 个序列，然后计算模型对这 n 个序列的 perplexity，我们认为其中 perplexity 最低的序列所对应的选项即为模型在这道题上面的推理结果，该种评测方法的后处理简单直接、确定性高。</p><p><code>gen</code> 是生成 (generate) 的缩写。在 OpenCompass 的语境下，它指的是在给定上下文的情况下，模型往后续写的结果就是这道题目上的推理结果。一般来说，续写得到的字符串需要结合上比较重的后处理过程，才能进行可靠的答案提取，从而完成评测。</p><p>从使用上来说，基座模型的单项选择题和部分具有选择题性质的题目会使用 <code>ppl</code>，基座模型的不定项选择和非选择题都会使用 <code>gen</code>。而对话模型的所有题目都会使用 <code>gen</code>，因为许多商用 API 模型不会暴露 <code>ppl</code> 的接口。但也存在例外情况，例如我们希望基座模型输出解题思路过程时 (例如 Let’s think step by step)，我们同样会使用 <code>gen</code>。</p><p>与 <code>ppl</code> 高度类似地，条件对数概率 <code>clp</code> (conditional log probability) 是在给定上下文的情况下，计算下一个 token 的概率。它也仅适用于选择题，考察概率的范围仅限于备选项标号所对应的 token，取其中概率最高的 token 所对应的选项为模型的推理结果。与 ppl 相比，<code>clp</code> 的计算更加高效，仅需要推理一次，而 ppl 需要推理 n 次，但坏处是，<code>clp</code> 受制于 tokenizer，在例如选项前后有无空格符号时，tokenizer 编码的结果会有变化，导致测试结果不可靠。因此 OpenCompass 中很少使用 <code>clp</code> 。</p><h3 id="OpenCompass-如何控制-few-shot-评测的-shot-数目？"><a href="#OpenCompass-如何控制-few-shot-评测的-shot-数目？" class="headerlink" title="OpenCompass 如何控制 few shot 评测的 shot 数目？"></a>OpenCompass 如何控制 few shot 评测的 shot 数目？</h3><p>在数据集配置文件中，有一个 <code>retriever</code> 的字段，该字段表示如何召回数据集中的样本作为上下文样例，其中最常用的是 <code>FixKRetriever</code> 表示固定使用某 k 个样本，因此即为 k-shot。另外还有 <code>ZeroRetriever</code> 表示不使用任何样本，这在大多数情况下意味着 0-shot。</p><h3 id="OpenCompass-task-的默认划分逻辑是什么样的？"><a href="#OpenCompass-task-的默认划分逻辑是什么样的？" class="headerlink" title="OpenCompass task 的默认划分逻辑是什么样的？"></a>OpenCompass task 的默认划分逻辑是什么样的？</h3><p>OpenCompass 默认使用 num_worker_partitioner。OpenCompass 的评测从本质上来说就是有一系列的模型和一系列的数据集，然后两两组合，用每个模型去跑每个数据集。对于同一个模型，OpenCompass 会将其拆分为 <code>--max-num-workers</code> (或 config 中的 <code>infer.runner.max_num_workers</code>) 个 task，为了保证每个 task 的运行耗时均匀，每个 task 均会所有数据集的一部分。</p><h2 id="MMBench-使用"><a href="#MMBench-使用" class="headerlink" title="MMBench 使用"></a>MMBench 使用</h2><p>OpenCompass 要求 <code>pytorch&gt;=1.13</code> 。</p><p>完整安装（支持更多数据集）<code>pip install &quot;opencompass[full]&quot;</code> </p><p>API 测试（例如 OpenAI &#x2F; Qwen）<code>pip install &quot;opencompass[api]&quot;</code></p><p>如果推理后端是 vllm ，那么用以下指令验证成功 <code>vllm serve facebook/opt-125m</code></p><p>针对 HumanEvalX &#x2F; HumanEval+ &#x2F; MBPP+ 需要手动克隆 git 仓库进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> --recurse-submodules git@github.com:open-compass/human-eval.git<br><span class="hljs-built_in">cd</span> human-eval<br>pip install -e .<br>pip install -e evalplus<br></code></pre></td></tr></table></figure><p>OpenCompass 支持的数据集主要包括三个部分：</p><ol><li><p>Huggingface 数据集： <a href="https://huggingface.co/datasets">Huggingface Dataset</a> 提供了大量的数据集，这部分数据集运行时会<strong>自动下载</strong>。</p></li><li><p>ModelScope 数据集：<a href="https://modelscope.cn/organization/opencompass">ModelScope OpenCompass Dataset</a> 支持从 ModelScope 自动下载数据集。</p><p>要启用此功能，请设置环境变量：<code>export DATASET_SOURCE=ModelScope</code>，可用的数据集包括（来源于 <code>OpenCompassData-core.zip</code>）：</p><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs mipsasm">humaneval, triviaqa, commonsenseqa, tydiqa, strategyqa, cmmlu, lambada, piqa, ceval, math, LCSTS, Xsum, winogrande, openbookqa, AGIEval, gsm8k, nq, race, siqa, mbpp, mmlu, hellaswag, ARC, <span class="hljs-keyword">BBH, </span>xstory_cloze, summedits, GAOKAO-<span class="hljs-keyword">BENCH, </span>OCNLI, cmnli<br></code></pre></td></tr></table></figure></li><li><p>自建以及第三方数据集：OpenCompass 还提供了一些第三方数据集及自建<strong>中文</strong>数据集。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>评估</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
