<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>【搬运】VLM简短综述</title>
    <link href="/2025/06/26/%E3%80%90%E6%90%AC%E8%BF%90%E3%80%91VLM%E7%AE%80%E7%9F%AD%E7%BB%BC%E8%BF%B0/"/>
    <url>/2025/06/26/%E3%80%90%E6%90%AC%E8%BF%90%E3%80%91VLM%E7%AE%80%E7%9F%AD%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="视觉语言模型：基本认识"><a href="#视觉语言模型：基本认识" class="headerlink" title="视觉语言模型：基本认识"></a>视觉语言模型：基本认识</h1><p>[TOC]</p><h2 id="1-定义和讨论范围"><a href="#1-定义和讨论范围" class="headerlink" title="1. 定义和讨论范围"></a>1. 定义和讨论范围</h2><p>定义：<font color=red>“Vision Language Models or VLMs are AI models that use both images and textual data to perform <u>tasks that fundamentally need both of them</u>.”</font> VLM 隶属于 AI 模型，这类 AI 模型使用图像或文本数据来执行<u>本质上需要这两个模态的任务</u>。</p><p>本文讨论的 VLM 特指于 “Our coverage will be exclusive to <u>VLMs that generate text as output</u>.” 也就是以文本<font color=blue>（包括自然语言、数字、代码）</font>为输出的 VLM 模型；而以图像为输出的模型此处不参与讨论。</p><h2 id="2-视觉语言模型适用任务"><a href="#2-视觉语言模型适用任务" class="headerlink" title="2. 视觉语言模型适用任务"></a>2. 视觉语言模型适用任务</h2><ol><li>Image Captioning: 生成描述图像的文字</li><li>Dense Captioning: 生成描述图像的、包含图像中突出特点&#x2F;特性的文字，相比于前者颗粒度更高</li><li>Instance Detection: 在图片中使用 bounding box 来标注出检测到的物体</li><li>Visual Question Answering (VQA): 针对图片进行文本模态的问与答</li><li>Image Retrieval or Text to Image discovery: 从多张图像中找到最匹配输入文本描述的图像</li><li>Zero Shot Image classification: 根据文本模态提供的类别信息直接对图像进行分类（无需额外训练）</li><li>Synthetic data generation: 根据 VLM 的特性制造合成数据</li></ol><h2 id="3-视觉语言模型发展历史"><a href="#3-视觉语言模型发展历史" class="headerlink" title="3. 视觉语言模型发展历史"></a>3. 视觉语言模型发展历史</h2><table><thead><tr><th>时间</th><th>2015 年前后</th><th>2020 年前后</th><th>2024年前后</th></tr></thead><tbody><tr><td>内容</td><td>出现了两项工作：<a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf">Show and Tell</a> 用于生成图像描述，和<a href="https://arxiv.org/pdf/1505.00468">Visual Question Answering</a> 用于生成视觉问答。</td><td>CLIP</td><td>ViT 和各种 LLM 的出现问世</td></tr><tr><td>意义</td><td>提出构建 VLM 的基本思路：<b>通过调整视觉主干的图像嵌入使其与文本主干兼容，促进视觉和文本表征之间的有效沟通。</b></td><td>1. 使用互联网的图文数据；2. 使用对比学习。</td><td>二者成为了当前 VLM 的基本组成部分。</td></tr></tbody></table><h2 id="4-视觉语言模型基本网络架构"><a href="#4-视觉语言模型基本网络架构" class="headerlink" title="4. 视觉语言模型基本网络架构"></a>4. 视觉语言模型基本网络架构</h2><p><img src="https://nanonets.com/blog/content/images/2024/09/image-171.png"></p><p>当前 VLM 的网络架构通常是以下三种：</p><ol><li><span style="background-color: yellow;">双塔 VLM </span>：视觉网络输出和文本网络输出的唯一连接位于整个 VLM 的最后一层。 </li><li><span style="background-color: yellow;">双腿 VLM </span>：单个 LLM 接受文本标记以及来自视觉编码器的标记。</li><li><span style="background-color: yellow;">统一&#x2F;单塔 VLM </span>：一个骨干同时处理视觉和文本输入。</li></ol><h2 id="5-视觉语言模型的模态融合方式"><a href="#5-视觉语言模型的模态融合方式" class="headerlink" title="5. 视觉语言模型的模态融合方式"></a>5. 视觉语言模型的模态融合方式</h2><h3 id="5-1-Shallow-Early-fusion-浅-早期融合"><a href="#5-1-Shallow-Early-fusion-浅-早期融合" class="headerlink" title="5.1 Shallow&#x2F;Early fusion 浅&#x2F;早期融合"></a>5.1 Shallow&#x2F;Early fusion 浅&#x2F;早期融合</h3><p>定义：视觉输入和语言输入之间的连接&#x2F;结合发生<u>在计算过程的早期</u>，这意味着视觉输入<u>在进入文本域之前只经历最少程度的转换</u>，因此称为 “浅” 。</p><p>【做法一】确保视觉编码器（vision encoder）输出能够与 LLMs 的输入兼容，并且<font color=red>仅训练视觉编码器，同时保持大语言模型的参数冻结</font>。该架构本质上是一个大语言模型<font color=blue>（通常是 decoder-only 语言模型）</font>，<u>它扩展出一个用于图像编码的分支</u>。这种架构编码实现简单、易于理解，通常不需要编写新的网络层。这类架构的损失函数与普通的大语言模型相同，即<u>关注下一个 token 的预测质量</u>。“Frozen” 就是这类实现的一个示例。除了训练视觉编码器，该方法还采用了 prefix-tuning 技术：为所有视觉输入附加一个静态的前缀 token。这种设置使得视觉编码器可以根据 LLM 对前缀的响应进行自我调整。</p><hr><p>【做法二】仅使用视觉编码器的问题在于，<font color=red>其输出很难确保与大语言模型（LLM）兼容</font>，从而限制了视觉模型与语言模型的可选组合。更简单的做法是，在视觉网络和语言模型之间加入一个<u>中间层</u><font color=blue>（包括投影层 projector 和适配器 adapter ）</font>，使视觉编码器的输出能够适配 LLM 。通过在两者之间插入中间层，可以将<b>任意</b>视觉嵌入对齐到<b>任意</b>语言模型可以理解的形式。这种架构在灵活性上<u>优于或等同于</u>训练视觉编码器。这样可以选择<u>同时冻结</u>视觉网络和语言模型，同时由于适配器模块通常<u>体积较小</u>，也能<u>加速训练过程</u>。代表的 VLMs 有：</p><ol><li><p><span style="background-color: yellow;">【简单中间层】</span>Llava 系模型；</p></li><li><p><span style="background-color: yellow;">【简单中间层】</span>Bunny: 有跨模态中间层 projector 的加持下，视觉输入可以使用多种编码器网络处理；文本输出可以使用多种语言模型产生；使用参数高效指令微调方法（LoRA）训练。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-140.png"></p></li><li><p><span style="background-color: yellow;">【简单中间层】</span>MM1: 使用混合专家模型 MoE</p></li><li><p><span style="background-color: yellow;">【简单中间层】</span>Cobra: 使用 Mamba 架构取代传统 Transformer ，同时使用<u>两个视觉编码器</u>联合提取图像表征。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-141.png"></p></li><li><p><span style="background-color: yellow;">【复杂中间层】</span>CLIP Cap: 视觉编码器采用 CLIP + 基于 Transformer 的映射网络，其中 Const. 是可学习常量。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-142.png"> </p></li><li><p><span style="background-color: yellow;">【复杂中间层】</span>BLIP-2: 使用 Q-Former 作为其中间层 projector ，以更有效地将图像内容与语言进行对齐和关联，实现更强的视觉语义对齐能力。</p></li><li><p><span style="background-color: yellow;">【复杂中间层】</span>MobileVLMv2: 使用一种基于<u>轻量级逐点卷积（point-wise convolution）</u>的架构，并以 MobileLLama 作为<u>小型语言模型（SLM）</u>替代传统的大语言模型（LLM），重点提升推理速度。</p></li><li><p><span style="background-color: yellow;">【多中间层联合】</span>BRAVE: 使用最多<b>五</b>个视觉编码器，并采用名为 MEQ-Former 的中间层，将所有视觉输入拼接为一个整体后再送入 VLM ，以实现<u>多视角信息</u>的融合与统一处理。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-143.png"></p></li><li><p><span style="background-color: yellow;">【多中间层联合】</span>Honeybee: 使用两个专用的视觉编码器，分别称为 C-Abstractor 和 D-Abstractor。C-Abstractor 注重保持图像的局部性信息，而 D-Abstractor 则具备输出可变数量 token 的能力，从而实现更灵活和精细的视觉信息表达。</p></li><li><p><span style="background-color: yellow;">【多中间层联合】</span>DeepSeek VL: 同样采用多个编码器，以保留图像中的高层语义信息和低层细节信息。然而，与其他方法不同的是，该模型还对 LLMs 进行联合训练，从而实现深度融合（deep fusion）。</p></li></ol><h3 id="5-2-Late-Fusion-后期融合"><a href="#5-2-Late-Fusion-后期融合" class="headerlink" title="5.2 Late Fusion 后期融合"></a>5.2 Late Fusion 后期融合</h3><p>这类架构中的<font color=red>视觉模型与文本模型是完全分离的</font>。图像和文本的嵌入<u>仅在损失计算阶段进行交互</u>，而这种损失通常是对比损失（contrastive loss）。</p><ol><li>CLIP 就是这类架构的经典代表，其中图像和文本分别独立编码，通过对比损失进行匹配，从而共同优化各自的编码器。</li><li>JINA CLIP 对传统 CLIP 架构进行了改进，不仅联合优化了 CLIP 损失（即 “图像—文本” 对比损失），还引入了 “文本—文本” 对比损失，其中只有语义相近的文本对才被视为相似。这其中的启示是：<span style="background-color: yellow;">通过引入更多的目标函数，可以使多模态对齐更加精准</span>。</li><li>ColPali 是另一种典型的后期融合模型，专门用于文档检索。它与 CLIP 略有不同，ColPali 使用视觉编码器与 LLMs 结合来生成视觉嵌入，而文本嵌入则完全依赖于 LLMs 。</li><li>ViTamin 训练了一个视觉主干网络，该网络将<u>卷积块</u>与 <u>Transformer 块</u>进行拼接融合，旨在兼具两者的优势，获得更优的视觉特征表示。</li></ol><h3 id="5-3-Deep-Fusion-深度融合"><a href="#5-3-Deep-Fusion-深度融合" class="headerlink" title="5.3 Deep Fusion 深度融合"></a>5.3 Deep Fusion 深度融合</h3><p>这类架构通常<b>在网络的深层</b>对图像特征进行注意力处理，从而实现更丰富的跨模态知识传递。训练过程一般覆盖所有模态，虽然训练时间较长，但往往能带来更高的效率和准确性。有时，这些架构类似于 Two-Leg VLMs 且 LLM 参数处于非冻结状态。</p><ol><li><p>CLIPPO 使用单个编码器同时处理视觉输入和文本输入。</p></li><li><p>Single-tower Transformer: 从头开始训练一个统一的 Transformer，使其能够同时支持多种 VLM 应用任务。</p></li><li><p>DINO: 在跨模态 Transformer 的基础上引入定位损失（localization loss），实现<u>零样本目标检测</u>，即能够预测训练集中未出现的类别。</p></li><li><p>KOSMOS-2: 将边界框（bounding boxes）作为输入和输出，与文本和图像的 token 一起处理，将目标检测能力直接内嵌到语言模型中。</p></li><li><p>Chameleon 通过使用量化器（quantizer）将图像原生地视作 token，从而构建出一种对文本与视觉模态均无偏向的通用架构。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-150.png"></p></li><li><p>FIBER 通过动态开关交叉注意力模块（cross attention modules）来执行不同任务，实现灵活的功能切换。</p></li><li><p>BridgeTower 构建了一个独立的跨模态编码器，内置 <u>“桥接层”（bridge-layer）</u>，能够在文本对视觉和视觉对文本的 token 之间进行交叉注意，捕捉更丰富的交互信息。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-152.png"></p></li><li><p>Flamingo 中，视觉 token 由<u>改进版 ResNet</u> 以及<u>类似 DETR 的专用层 Perceiver Resampler</u> 生成。随后，利用 Chinchilla LLM 作为<u>冻结</u>骨干，通过跨注意力将视觉 token 与语言 token 进行密集融合。</p></li><li><p>MoE-LLaVa 则采用 MoE 技术处理视觉和文本 token，训练分为两个阶段：首先仅训练前馈网络（FFNs），随后训练大语言模型（LLM）。</p></li></ol><h2 id="6-视觉语言模型的训练过程"><a href="#6-视觉语言模型的训练过程" class="headerlink" title="6. 视觉语言模型的训练过程"></a>6. 视觉语言模型的训练过程</h2><h3 id="6-1-训练目标"><a href="#6-1-训练目标" class="headerlink" title="6.1 训练目标"></a>6.1 训练目标</h3><h4 id="6-1-1-Contrastive-Loss-对比损失"><a href="#6-1-1-Contrastive-Loss-对比损失" class="headerlink" title="6.1.1 Contrastive Loss 对比损失"></a>6.1.1 Contrastive Loss 对比损失</h4><p>这种方法旨在调整<u>嵌入表征</u>，使<font color=red>匹配对之间的距离最小化，而非匹配对之间的距离最大化</font>。这种对比学习方法尤为有效，因为匹配对易于获取，而且还能<b>指数级地</b>增加可用于训练的<b>负样本</b>数量。CLIP 及其各种变体就是采用对比损失进行训练的经典范例，匹配关系发生在图像与文本的嵌入之间。InternVL、BLIP2 和 SigLIP 也是一些值得关注的相关模型。</p><p>SLIP 展示了在进行 CLIP 预训练之前，先对视觉编码器进行 “图像—图像” 的对比学习预训练，可以显著提升整体性能。Florence 对对比损失进行了改进，将图像标签和文本的哈希值引入其中，形成了一种称为 Unified-CL 的方法。ColPali 则结合了两种对比损失：一是 “图像—文本” 的对比损失，二是 “文本—文本” 的对比损失。</p><h4 id="6-1-2-Generative-Loss-生成式损失"><a href="#6-1-2-Generative-Loss-生成式损失" class="headerlink" title="6.1.2 Generative Loss 生成式损失"></a>6.1.2 Generative Loss 生成式损失</h4><p>这一类损失函数将 VLM 视为生成器，通常用于零样本任务和语言生成任务。</p><ul><li>语言建模损失（Language Modeling Loss）：这是在训练 VLM 进行下一个 token 预测时最常用的损失函数。Chameleon 对该损失进行了扩展，既用于预测文本 token，也用于预测图像 token。Florence2 则在所有任务中统一使用这种损失函数。</li><li>掩码<b>语言</b>建模（Masked Language Modeling, MLM）：通过遮蔽部分文本 token 并让模型在上下文中进行预测，从而训练文本编码器。FIBER 就是使用这一方法的众多模型之一。</li><li>掩码<b>图像</b>建模（Masked Image Modeling, MIM）：通过遮蔽输入图像中的一部分 token，让 Transformer 预测被遮蔽的信息，从而迫使模型在有限数据下学习。代表性方法包括：LayoutLM、SegCLIP 中使用的 MAE（Masked Autoencoder）、FLAVA 中采用的 BeiT 等。</li><li>掩码<b>图像+文本</b>建模（Masked Image+Text Modeling）：顾名思义，这种方法在图像和文本两个模态中同时进行 token 的遮蔽，以最大限度地在有限数据下促进跨模态交互学习。FLAVA 就是这类方法的代表。</li></ul><h4 id="6-1-3-Niche-Cross-Modality-Alignments-混合跨模态对齐"><a href="#6-1-3-Niche-Cross-Modality-Alignments-混合跨模态对齐" class="headerlink" title="6.1.3 Niche Cross Modality Alignments 混合跨模态对齐"></a>6.1.3 Niche Cross Modality Alignments 混合跨模态对齐</h4><p>鉴于多模态任务的多样性，研究者常常可以设计出具有针对性的目标函数来实现更精细的模态对齐。例如：</p><ul><li><b>BLIP2</b> 提出了<u>图像引导的文本生成损失（Image-grounded Text Generation Loss）</u>，通过图像内容引导生成更贴合语义的文本。</li><li><b>LayoutLM</b> 使用了一种称为<u>词块对齐（Word Patch Alignment）</u>的机制，用于粗略识别文档中每个词所在的位置，从而将文本与图像中的文档结构进行对齐。</li></ul><h3 id="6-2-训练视觉语言模型的实践"><a href="#6-2-训练视觉语言模型的实践" class="headerlink" title="6.2 训练视觉语言模型的实践"></a>6.2 训练视觉语言模型的实践</h3><ul><li><p>预训练（Pre-training）</p><p>在这一阶段，通常仅训练 adapter 或者 projector ，并使用尽可能大量的数据（往往是数百万对 “图像—文本” 对）。目标是使<b>图像编码器与文本解码器对齐</b>，此阶段的重点在于<b>数据量的规模</b>。训练通常是无监督的，常用的损失函数包括 <b>contrastive loss</b> 或 <b>next token prediction loss</b> ，并通过调整输入文本 prompt，使语言模型能更好地理解图像上下文。</p></li><li><p>微调（Fine-tuning）</p><p>根据模型架构的不同，有些或全部模块（适配器、文本模型、视觉模型）从一开始就解冻参与训练。由于参数规模庞大，训练过程会非常缓慢。因此，此阶段使用的数据量通常仅是预训练阶段的一小部分，并且<b>每条数据都需具备极高质量</b>。</p></li><li><p>指令微调（Instruction Tuning）</p><p>这一阶段通常是训练流程中的第二步或第三步，旨在将模型调整为可用于<u>聊天&#x2F;问答</u>等交互式任务。数据会被整理成 “指令—响应” 的格式，常通过现有 LLMs 将普通数据转化为指令形式。<b>LLaVa</b> 和 <b>Vision-Flan</b> 是典型例子。</p></li><li><p>使用 LoRA（Low-Rank Adaptation）</p><p>如前所述，微调过程中可能需要解冻 LLM 参数，而这是一项极其昂贵的操作。LoRA 提供了一种高效替代方案：通过在 LLM 各层之间插入小型的<u>低秩适配层</u>，能够在保持整体调整的同时，只训练 LLM 的一小部分参数，显著降低训练成本。</p></li><li><p>多分辨率输入（Multiple Resolutions）</p><p>在处理包含高密度信息的图像任务时（例如目标计数、拥挤人群识别或 OCR 中文字识别），VLMs 往往面临挑战。因此，支持多分辨率输入成为提高模型鲁棒性和适应能力的一项关键设计。</p><p>最简单的方式是将图像调整为多个不同的分辨率，并从每个分辨率中裁剪出所有子图（crops），然后将这些子图输入视觉编码器，并作为 token 喂入 LLM 。这一策略由 Scaling on Scales 提出，并被 Bunny 系列模型广泛采用，该系列在多项任务中表现出色。</p><p>LLaVA-UHD 则尝试寻找最优的图像切片（grid 划分）策略，在输入视觉编码器之前对图像进行合理划分。</p><p><img src="https://i.imgur.com/IcKbERN.png"></p><p>InternLMXComposer2-4KHD 采用动态多分辨率策略，将图像在多个分辨率下裁剪得到的所有子图全部输入视觉编码器，从而更充分地捕捉图像中的细节信息。</p></li></ul><h3 id="6-3-训练数据集"><a href="#6-3-训练数据集" class="headerlink" title="6.3 训练数据集"></a>6.3 训练数据集</h3><table><thead><tr><th align="left">数据集名称和年份</th><th align="left">图文对数量</th><th align="left">数据集描述</th></tr></thead><tbody><tr><td align="left">WebLI (2022)</td><td align="left">12B</td><td align="left">基于 109 种语言的，网络爬取得到的，最大数据集之一。<font color=red>这不是公共数据集。</font></td></tr><tr><td align="left"><a href="https://laion.ai/blog/laion-5b/">LAION-5B</a> (2022)</td><td align="left">5.5B</td><td align="left">基于互联网上收集得到的图像-文本对。<font color=red>最大的公开可用数据集之一</font>，已被用来实现从头开始预训练 VLMs 。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2311.06242">FLD-5B</a> (2023)</td><td align="left">5B</td><td align="left">密集描述（dense captions）、OCR 信息以及定位信息（包括分割掩码和多边形轮廓）为构建一个统一的多任务学习模型提供了理想的基础。这种设计有助于模型在处理多种任务时实现知识共享，并显著提升其泛化能力。</td></tr><tr><td align="left"><a href="https://github.com/kakaobrain/coyo-dataset">COYO</a> (2022)</td><td align="left">700M</td><td align="left">另一个重要的数据集。通过图像级和文本级的筛选流程，过滤掉无信息量的图文对。</td></tr><tr><td align="left"><a href="https://laion.ai/blog/laion-coco/">LAION-COCO</a> (2022)</td><td align="left">600M</td><td align="left">它是 LAION-5B 的一个子集，并使用合成手段生成的图像描述，因为原始的 alt-text 往往并不准确。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2306.16527">Obelics</a> (2023)</td><td align="left">141M</td><td align="left">该数据集采用<b>对话</b>格式，即图像与文本组成的多轮对话，非常适用于指令预训练和指令微调任务。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2304.06939">MMC4 (Interleaved)</a> (2023)</td><td align="left">101M</td><td align="left">同样是聊天格式，该数据集还使用<u>线性分配算法（linear assignment algorithm）</u>，借助 CLIP 特征将图像合理地插入到较长的文本中，增强图文之间的上下文关联性。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/1503.01817v2">Yahoo Flickr Creative Commons 100 Million (YFCC100M)</a> (2016)</td><td align="left">100M</td><td align="left">这是最早期的大规模多模态数据集之一。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2103.01913">Wikipedia-based Image Text</a> (2021)</td><td align="left">37M</td><td align="left">其独特之处在于将图像与百科式知识关联起来。</td></tr><tr><td align="left"><a href="https://github.com/google-research-datasets/conceptual-12m">Conceptual Captions (CC12M)</a> (2021)</td><td align="left">12M</td><td align="left">与多数只关注现实世界物体或事件的数据集不同，它涵盖了更大范围、更具多样性的概念。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2111.11431">Red Caps</a> (2021)</td><td align="left">12M</td><td align="left">该数据集采集自 Reddit，因此其 captions 反映了真实世界中用户生成的内容，涵盖多种类别，具有更高的真实性与多样性，相比标准数据集更加贴近<u>自然语言</u>使用方式。</td></tr><tr><td align="left"><a href="https://homes.cs.washington.edu/~ranjay/visualgenome/index.html">Visual Genome</a> (2017)</td><td align="left">5.4M</td><td align="left">该数据集还包含丰富的标注信息，包括物体检测、物体之间的关系、以及场景中物体的属性，使其非常适合用于场景理解与 dense captioning 等任务。</td></tr><tr><td align="left"><a href="https://ai.google.com/research/ConceptualCaptions/download">Conceptual Captions (CC3M)</a> (2018)</td><td align="left">3.3M</td><td align="left">该数据集更适合微调。</td></tr><tr><td align="left"><a href="https://huggingface.co/datasets/BoyaWu10/Bunny-v1_1-data">Bunny-pretrain-LAION-2M</a> (2024)</td><td align="left">2M</td><td align="left">该数据集强度 “视觉—文本” 的对齐。</td></tr><tr><td align="left"><a href="https://sharegpt4v.github.io/">ShareGPT4V-PT</a> (2024)</td><td align="left">1.2M</td><td align="left">该数据集源自 ShareGPT 平台，其 captions 由一个模型生成，而该模型本身是基于 GPT-4V 生成的描述数据进行训练的。这使得数据集中的文本具有较高的语言质量与视觉理解能力，适用于多模态模型的高质量指令微调和评估。</td></tr><tr><td align="left"><a href="https://www.cs.rice.edu/~vo9/sbucaptions/">SBU Caption</a> (2011)</td><td align="left">1M</td><td align="left">该数据集采集自 Flickr ，适用于研究日常场景中图像与文本之间的自然对应关系。它以休闲、生活化的图像为主，能够支持模型在处理非专业、真实用户生成内容时的表现与泛化能力。</td></tr><tr><td align="left"><a href="http://cocodataset.org/#home">COCO Caption</a> (2016)</td><td align="left">1M</td><td align="left">每张图片均有 5 个独立的人类进行描述。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/1912.03098">Localized Narratives</a> (2020)</td><td align="left">870k</td><td align="left">该数据集包含定位到具体物体的描述信息，非常适合用于图像定位（image grounding）等任务。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2402.11684">ALLaVA-Caption-4V</a> (2024)</td><td align="left">715k</td><td align="left">该数据集的图像描述由 GPT-4V 生成，主要聚焦于图像描述和视觉推理任务。</td></tr><tr><td align="left"><a href="https://github.com/haotian-liu/LLaVA">LLava-1.5-PT</a> (2024)</td><td align="left">558k</td><td align="left">另一个通过调用 GPT-4 生成的数据集，重点在于提供高质量的提示 prompts ，以支持视觉推理和密集描述任务。</td></tr><tr><td align="left"><a href="https://arxiv.org/pdf/2007.00398">DocVQA</a> (2021)</td><td align="left">50k</td><td align="left">这是一个基于文档的视觉问答数据集，问题主要聚焦于文档内容，因而在金融、法律或行政等领域的信息抽取任务中具有重要价值。</td></tr></tbody></table><h2 id="7-视觉语言模型评估"><a href="#7-视觉语言模型评估" class="headerlink" title="7. 视觉语言模型评估"></a>7. 视觉语言模型评估</h2><ul><li><p>MMMU（Massive Multi-discipline Multimodal Understanding and Reasoning）</p><p>MMMU 基准测试是当前评估 VLMs 最受欢迎的基准之一。该基准涵盖多个学科领域，旨在测试 VLMs 的泛化能力，确保优秀的 VLMs 能够跨领域表现出稳定性能。该基准主要评估三项核心能力：<u>感知能力（perception）、知识理解（knowledge）和推理能力（reasoning）</u>。评估采用<u>零样本（zero-shot）</u>设定，要求模型在不进行微调或少样本提示的情况下直接生成准确答案，真正考验其泛化和理解能力。MMMU-PRO 是该基准的增强版本，在原有基础上引入了更具挑战性的问题，并剔除了一些仅凭文本输入就能轻松解答的数据点，从而提升了整体评估的难度与区分度。</p></li><li><p>MME</p><p>MME 数据集高度注重质量，所有图像均为精心挑选并人工标注，并确保这些图像和问题在互联网上<b>完全不可获取</b>，以避免 VLMs 在训练阶段意外接触这些样本，从而保障评估的公正性与准确性。该基准包含 <b>14</b> 个子任务，每个任务约含 50 张图像，每个任务的回答均为<b>是&#x2F;否（Yes&#x2F;No）</b>形式。一些典型任务包括：<u>物体是否存在、对知名人物或物品的感知、文本翻译</u>等。每张图像都配有两道问题：一道是正向问题，期望模型回答 “YES” ；一道反向问题，期望模型回答 “NO” 。每个子任务都构成一个独立的评估基准。除此之外，还有两个子聚合基准：认知子基准（Cognition Benchmark）：聚合所有推理类子任务的准确率；感知子基准（Perception Benchmark）：聚合所有感知类子任务的准确率。最终的总体基准得分为上述所有子任务基准的总和，用于全面衡量模型在认知与感知方面的综合能力。</p><p><img src="https://i.imgur.com/93kXUPG.png"></p></li><li><p>MMStar</p><p>该数据集是从 6 个 VQA 数据集中精挑细选出的一个高质量子集，经过严格筛选以确保以下几点：（1）<u>无法仅凭 LLMs 的文本模态知识作答</u>，图像信息是不可或缺的。（2）<u>问题本身不包含答案</u>，确保图像在回答过程中不是可有可无的。（3）<u>问题与答案的文本内容未直接出现在 LLMs 的训练语料中</u>，防止模型通过记忆作答。与 MME 类似，该数据集强调的是质量而非数量，适用于更严谨和真实的多模态模型评估。</p><p><img src="https://nanonets.com/blog/content/images/2024/09/image-155.png"></p></li><li><p>Math-Vista</p><p>该基准测试数据集从 31 个不同来源收集并精心整理而成，专注于数学相关问题，涵盖了多种推理类型、任务形式、年级水平和语境场景。其多样性使其成为评估模型在数学理解与推理能力方面的重要工具。</p></li><li><p>MathVerse</p><p>MathVerse 是一个与上述数学基准类似但有所不同的数据集，主要聚焦于更具体的数学领域，如<u>二维几何、三维几何以及解析函数</u>等内容。该数据集更强调视觉与空间推理能力，适用于评估模型在几何与函数理解方面的多模态能力。</p></li><li><p>AI2D</p><p>这是一个高度专注于科学图示的数据集，旨在评估 VLMs 对高级科学概念的理解能力。模型不仅需要解析图中的各个元素，还要理解它们之间的相对位置关系、箭头指向关系，以及每个组成部分所附带的文本信息。该数据集包含 5000 张小学科学图示，配有超过 150,000 条丰富注释、对应的句法解析（syntactic parses），以及 15000 多个多项选择题，为模型提供了全面的科学图示理解和推理评估基准。</p><p><img src="https://i.imgur.com/ULNHUwJ.png"></p></li><li><p>ScienceQA</p><p>这是另一个面向特定领域的数据集，专门用于评估模型在思维链范式下的表现。它不仅要求模型选择正确的多项选择题答案，还需生成详尽的推理解释，以验证其推理深度和逻辑连贯性。此外，该数据集还测试模型的对话式交互能力，通过输入多段文本和多张图像，评估其在复杂上下文中的理解与应答能力，是对多模态模型综合语言推理与多轮对话处理能力的有力检验。</p></li><li><p>MM-Vet v2</p><p>这是最受欢迎、同时也是<b>体量最小</b>的基准测试之一。该数据集通过评估 “单图—单文本” 和类聊天场景下的多种任务，全面考察模型的以下能力：识别能力、常识与知识理解、空间感知、语言生成、光学字符识别和数学推理能力。在该基准上，InternVL 是开源模型中表现最优秀的之一，再次展现其在多模态理解方面的强大能力。</p></li><li><p>VisDial</p><p>VisDial 一个源自 COCO 数据集的多模态数据集，旨在评估 VLM 聊天机器人在面对一系列图像与文本输入后，对后续问题的回答能力。它更关注于对<u>上下文图文交互的理解与连续对话</u>能力。</p></li><li><p>LLaVA-NeXT-Interleave</p><p>LLaVA-NeXT-Interleave 一个评估模型在多图输入情境下表现的基准测试。它整合了 9 个新数据集和 13 个已有数据集（包括 Muir-Bench 和 ReMI），全面考察模型在处理多张图像时的整合理解能力、跨图推理能力与一致性判断能力。</p></li><li><p>SEED</p><p>具有图像和视频模态的多项选择题。</p></li><li><p>VQA</p><p>最早的数据集之一，覆盖了广泛的日常场景。</p></li><li><p>GQA</p><p>GQA 是一个专注于组合式问答的数据集，其问题通常涉及<u>图像中多个物体之间的关系与属性组合</u>。例如，问题可能要求模型理解 “左边的红色物体是否在蓝色桌子上？” 这类需要多层次推理的问题。GQA 强调模型的<u>逻辑推理能力、结构化理解以及对复杂视觉语义</u>的掌握。</p></li><li><p>VisWiz</p><p>VisWiz 是一个由<b>盲人用户</b>生成的数据集。每位盲人拍摄一张图片<font color=blue>（啊？？？？？？）</font>，并针对该图片录制一段口述提问，随后通过众包方式为每个视觉问题收集了约 10 个答案。该数据集反映了真实用户在视觉辅助场景下的需求，特别适用于评估模型在辅助盲人理解视觉内容方面的能力。</p></li><li><p>POPE </p><p>POPE 是一个有趣的数据集，展示了如何通过简单的构建模块创造复杂的测试场景。它的问题形式主要是判断图像中某些物体的存在或不存在，首先利用目标检测模型识别图像中的物体作为 “存在” 样本，再通过否定方式生成 “缺失” 或 “不存在” 的样本集。该数据集不仅用于考察模型对物体存在性的理解能力，还被用于检测 VLMs 是否出现 “幻觉” 现象，即模型对不存在物体的错误预测。</p></li></ul><h2 id="8-训练-VLMs-需要关注的问题"><a href="#8-训练-VLMs-需要关注的问题" class="headerlink" title="8. 训练 VLMs 需要关注的问题"></a>8. 训练 VLMs 需要关注的问题</h2><h3 id="8-1-明确任务需求"><a href="#8-1-明确任务需求" class="headerlink" title="8.1 明确任务需求"></a>8.1 明确任务需求</h3><ul><li>明确模型用途：只做 VQA ？还是需要图像检索、grounding 等功能？</li><li>交互方式：单张图片输入还是支持对话式交互？</li><li>性能要求：是否实时响应？客户端能否等待？</li><li>任务不同，模型大小和架构选择不同。</li></ul><h3 id="8-2-数据与模型选择"><a href="#8-2-数据与模型选择" class="headerlink" title="8.2 数据与模型选择"></a>8.2 数据与模型选择</h3><ul><li>先用现有 SOTA 做 zero-shot 或 one-shot 测试，验证数据适配性。</li><li>数据复杂或专业时，评估数据量，决定是从头训练还是微调已有模型。</li><li>数据量不足时，考虑用现有 LLMs（GPT、Gemini、Claude等）生成高质量合成数据扩充。</li></ul><h3 id="8-3-损失函数设计"><a href="#8-3-损失函数设计" class="headerlink" title="8.3 损失函数设计"></a>8.3 损失函数设计</h3><ul><li>设计多目标损失函数确保任务被充分表达，借鉴 CLIP 变体、LayoutLM、BLIP-2 等多损失训练方法。</li><li>好的损失设计可以显著提升模型效果。</li></ul><h3 id="8-4-评估与业务指标"><a href="#8-4-评估与业务指标" class="headerlink" title="8.4 评估与业务指标"></a>8.4 评估与业务指标</h3><ul><li>选择适合的公开 benchmark 进行评测。</li><li>制定符合实际业务需求的指标，不能只依赖损失或 benchmark 成绩。</li><li><span style="background-color: yellow;">业务指标才是真正决定模型是否可用的标准</span>。</li></ul><h3 id="8-5-微调策略"><a href="#8-5-微调策略" class="headerlink" title="8.5 微调策略"></a>8.5 微调策略</h3><ul><li>第一阶段只训练 adapter 模块。</li><li>第二阶段用 LoRA 技术联合训练视觉编码器和语言模型。</li><li>保证数据质量，避免劣质样本拖累训练。</li></ul><h3 id="8-6-从零训练策略"><a href="#8-6-从零训练策略" class="headerlink" title="8.6 从零训练策略"></a>8.6 从零训练策略</h3><ul><li>选择适合领域的骨干网络。</li><li>采用多分辨率技术捕捉图像多层次细节。</li><li>使用多个视觉编码器融合信息。</li><li>对复杂数据，采用 MoE 策略训练语言模型。</li></ul><h3 id="8-7-高级实践"><a href="#8-7-高级实践" class="headerlink" title="8.7 高级实践"></a>8.7 高级实践</h3><ul><li>先训练超大模型（50亿参数以上），再用知识蒸馏（distillation）缩小模型。</li><li>试验多种架构组合，形成 “模型家族” ，选择最优组合。</li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>综述</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记-第3篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC3%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC3%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第二代-Qwen2-VL-应用案例"><a href="#第二代-Qwen2-VL-应用案例" class="headerlink" title="第二代 Qwen2-VL 应用案例"></a>第二代 Qwen2-VL 应用案例</h1><h2 id="模型下载-——-基于魔塔社区"><a href="#模型下载-——-基于魔塔社区" class="headerlink" title="模型下载 —— 基于魔塔社区"></a>模型下载 —— 基于魔塔社区</h2><p>本次 Qwen2-VL 开源了两个尺寸的模型，<strong>Qwen2-VL-2B-Instruct</strong> 和 <strong>Qwen2-VL-7B-Instruct</strong>，以及其 GPTQ 和 AWQ 的量化版本。</p><p><strong>模型链接：</strong></p><p>Qwen2-VL-2B-Instruct：<a href="https://www.modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct">https://www.modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct</a></p><p>Qwen2-VL-7B-Instruct：<a href="https://www.modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct">https://www.modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct</a></p><p>推荐使用 ModelScope CLI 下载模型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modelscope download --model=qwen/Qwen2-VL-7B-Instruct --local_dir ./Qwen2-VL-7B-Instruct<br></code></pre></td></tr></table></figure><h2 id="模型推理-——-基于-transformers"><a href="#模型推理-——-基于-transformers" class="headerlink" title="模型推理 —— 基于 transformers"></a>模型推理 —— 基于 transformers</h2><p>安装依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install git+https://github.com/huggingface/transformers<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install qwen-vl-utils<br></code></pre></td></tr></table></figure><h3 id="单图推理"><a href="#单图推理" class="headerlink" title="单图推理"></a>单图推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor<br><span class="hljs-keyword">from</span> qwen_vl_utils <span class="hljs-keyword">import</span> process_vision_info<br><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br><br><br>model_dir = <span class="hljs-string">&quot;/mnt/workspace/Qwen2-VL-2B-Instruct&quot;</span><br>min_pixels = <span class="hljs-number">256</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br>max_pixels = <span class="hljs-number">1280</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br><br><span class="hljs-comment"># Load the model in half-precision on the available device(s)</span><br>model = Qwen2VLForConditionalGeneration.from_pretrained(<br>    model_dir, device_map=<span class="hljs-string">&quot;auto&quot;</span>, torch_dtype = torch.float16<br>)<br><span class="hljs-comment"># Load the processor</span><br>processor = AutoProcessor.from_pretrained(<br>model_dir, min_pixels=min_pixels, max_pixels=max_pixels<br>)<br><br>messages = [<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <br>        <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;</span>&#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Describe this image.&quot;</span>&#125;<br>        ]<br>    &#125;<br>]<br><br><span class="hljs-comment"># Preparation for inference</span><br>text = processor.apply_chat_template(<br>    messages, tokenize=<span class="hljs-literal">False</span>, add_generation_prompt=<span class="hljs-literal">True</span><br>)<br>image_inputs, video_inputs = process_vision_info(messages)<br>inputs = processor(<br>text=[text], images=image_inputs, videos=video_inputs, <br>    padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span><br>)<br>inputs = inputs.to(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><br><span class="hljs-comment"># Inference: Generation of the output</span><br>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">128</span>)<br>generated_ids_trimmed = [<br>    out_ids[<span class="hljs-built_in">len</span>(in_ids):] <span class="hljs-keyword">for</span> in_ids, out_ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(inputs.input_ids, generated_ids)<br>]<br><br>output_text = processor.batch_decode(<br>generated_ids_trimmed, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span><br>)<br><br><span class="hljs-built_in">print</span>(output_text)<br></code></pre></td></tr></table></figure><p>把输入文本编程对话形式：输入文本被处理成对话模板的样式，图片或者视频暂时被 <code>&lt;|image_pad|&gt;</code> 这样的占位符替代。</p><figure class="highlight tex"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs tex"> &lt;|im<span class="hljs-built_in">_</span>start|&gt;system<br>You are a helpful assistant.&lt;|im<span class="hljs-built_in">_</span>end|&gt;<br>&lt;|im<span class="hljs-built_in">_</span>start|&gt;user<br>&lt;|vision<span class="hljs-built_in">_</span>start|&gt;&lt;|image<span class="hljs-built_in">_</span>pad|&gt;&lt;|vision<span class="hljs-built_in">_</span>end|&gt;Describe this image.&lt;|im<span class="hljs-built_in">_</span>end|&gt;<br>&lt;|im<span class="hljs-built_in">_</span>start|&gt;assistant<br></code></pre></td></tr></table></figure><h3 id="多图推理"><a href="#多图推理" class="headerlink" title="多图推理"></a>多图推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Messages containing multiple images and a text query</span><br>messages = [<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <br>        <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-string">&quot;file:///path/to/image1.jpg&quot;</span>&#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-string">&quot;file:///path/to/image2.jpg&quot;</span>&#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Identify the similarities between these images.&quot;</span>&#125;<br>        ]<br>    &#125;<br>]<br></code></pre></td></tr></table></figure><h3 id="视频理解"><a href="#视频理解" class="headerlink" title="视频理解"></a>视频理解</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Messages containing a video and a text query</span><br>messages = [<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <br>        <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;video&quot;</span>, <br>                <span class="hljs-string">&quot;video&quot;</span>: <span class="hljs-string">&quot;file:///path/to/video1.mp4&quot;</span>, <br>                <span class="hljs-string">&#x27;max_pixels&#x27;</span>: <span class="hljs-number">360</span>*<span class="hljs-number">420</span>, <br>                <span class="hljs-string">&#x27;fps&#x27;</span>: <span class="hljs-number">1.0</span><br>            &#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Describe this video.&quot;</span>&#125;<br>        ]<br>    &#125;<br>]<br></code></pre></td></tr></table></figure><h2 id="模型推理-——-基于-vllm"><a href="#模型推理-——-基于-vllm" class="headerlink" title="模型推理 —— 基于 vllm"></a>模型推理 —— 基于 vllm</h2><p>安装依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install git+https://github.com/fyabc/vllm.git@add_qwen2_vl_new<br></code></pre></td></tr></table></figure><p>启动 OpenAI 接口服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-VL-7B-Instruct --model model_path<br></code></pre></td></tr></table></figure><p>调用服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl http://localhost:8000/v1/chat/completions \    <br>-H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \    <br>-d <span class="hljs-string">&#x27;&#123;    </span><br><span class="hljs-string">&quot;model&quot;: &quot;Qwen2-VL-7B-Instruct&quot;,    </span><br><span class="hljs-string">&quot;messages&quot;: [    </span><br><span class="hljs-string">&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,    </span><br><span class="hljs-string">&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [        </span><br><span class="hljs-string">&#123;&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: &#123;&quot;url&quot;: &quot;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&quot;&#125;&#125;,        </span><br><span class="hljs-string">&#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What is the text in the illustrate?&quot;&#125;    </span><br><span class="hljs-string">]&#125;    </span><br><span class="hljs-string">]    </span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>调用服务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br><br><span class="hljs-comment"># Set OpenAI&#x27;s API key and API base to use vLLM&#x27;s API server.</span><br>openai_api_key = <span class="hljs-string">&quot;EMPTY&quot;</span><br>openai_api_base = <span class="hljs-string">&quot;http://localhost:8000/v1&quot;</span><br><br>client = OpenAI(<br>api_key=openai_api_key, base_url=openai_api_base<br>)<br><br>chat_response = client.chat.completions.create(<br>model=<span class="hljs-string">&quot;Qwen2-7B-Instruct&quot;</span>,    <br>    messages=[<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>&#125;,<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image_url&quot;</span>, <span class="hljs-string">&quot;image_url&quot;</span>: &#123;<span class="hljs-string">&quot;url&quot;</span>: <span class="hljs-string">&quot;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&quot;</span>&#125;&#125;,<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What is the text in the illustrate?&quot;</span>&#125;,        <br>        ]&#125;,<br>]<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Chat response:&quot;</span>, chat_response)<br></code></pre></td></tr></table></figure><h2 id="模型微调-——-基于-swift"><a href="#模型微调-——-基于-swift" class="headerlink" title="模型微调 —— 基于 swift"></a>模型微调 —— 基于 swift</h2><p>在开始微调之前，请确保您的环境已准备妥当。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/modelscope/swift.gitcd swift<br></code></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> -e .[llm]<br></code></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> pyav qwen_vl_utils<br></code></pre></td></tr></table></figure><h3 id="图像描述微调"><a href="#图像描述微调" class="headerlink" title="图像描述微调"></a>图像描述微调</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 默认会将 lora_target_modules 设置为 llm 的所有linear</span><br>CUDA_VISIBLE_DEVICES=<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span> NPROC_PER_NODE=<span class="hljs-number">4</span> swift sft \  <br>--model_type qwen2-vl-7b-instruct \  <br>    --model_id_or_path qwen/Qwen2-VL-7B-Instruct \  <br>    --sft_type lora \  <br>    --dataset coco-en-mini<span class="hljs-comment">#20000 \  </span><br>    --deepspeed default-zero2<br>    --dataset train.jsonl \  <br>    --val_dataset val.jsonl \  <span class="hljs-comment"># 自定义数据集</span><br></code></pre></td></tr></table></figure><p>以下是自定义数据集的样例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;image&gt;55555&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;66666&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;image_path&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;eeeee&lt;image&gt;eeeee&lt;image&gt;eeeee&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fffff&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;image_path1&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;image_path2&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;EEEEE&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;FFFFF&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;query1&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;response2&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;query2&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;response2&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>微调后推理脚本如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift infer \    <br>--ckpt_dir output/qwen2-vl-7b-instruct/vx-xxx/checkpoint-xxx \    <br>--load_dataset_config <span class="hljs-literal">true</span> <br>--merge_lora <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h3 id="图像-grounding-微调"><a href="#图像-grounding-微调" class="headerlink" title="图像 grounding 微调"></a>图像 grounding 微调</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs json"># swift 跨模型通用格式<br><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Find &lt;bbox&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;ref-object&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;/coco2014/train2014/COCO_train2014_000000001507.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;objects&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;[&#123;\&quot;caption\&quot;: \&quot;guy in red\&quot;, \&quot;bbox\&quot;: [138, 136, 235, 359], \&quot;bbox_type\&quot;: \&quot;real\&quot;, \&quot;image\&quot;: 0&#125;]&quot;</span> <br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Find &lt;ref-object&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;bbox&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;/coco2014/train2014/COCO_train2014_000000001507.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;objects&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;[&#123;\&quot;caption\&quot;: \&quot;guy in red\&quot;, \&quot;bbox\&quot;: [138, 136, 235, 359], \&quot;bbox_type\&quot;: \&quot;real\&quot;, \&quot;image\&quot;: 0&#125;]&quot;</span> <br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"># qwen2-vl-chat 特定格式，注意特殊字符的存在<br><span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Find &lt;|object_ref_start|&gt;the man&lt;|object_ref_end|&gt;&quot;</span><span class="hljs-punctuation">,</span> <br><span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;|box_start|&gt;(123,235),(324,546)&lt;|box_end|&gt;&quot;</span><span class="hljs-punctuation">,</span> <br><span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;/coco2014/train2014/COCO_train2014_000000001507.jpg&quot;</span><span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><h3 id="视频微调"><a href="#视频微调" class="headerlink" title="视频微调"></a>视频微调</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">NFRAMES=24 MAX_PIXELS=100352 CUDA_VISIBLE_DEVICES=0,1,2,3 NPROC_PER_NODE=4 swift sft \  <br>--model_type qwen2-vl-7b-instruct \  <br>--model_id_or_path qwen/Qwen2-VL-7B-Instruct \  <br>--sft_type lora \  <br>--dataset video-chatgpt \  <br>--deepspeed default-zero2<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记-第2篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第二代-Qwen2-VL-2024-09-2024-10"><a href="#第二代-Qwen2-VL-2024-09-2024-10" class="headerlink" title="第二代 Qwen2-VL 2024.09-2024.10"></a>第二代 Qwen2-VL 2024.09-2024.10</h1><blockquote><p>参考网页：</p><p><a href="https://zhuanlan.zhihu.com/p/7352653203">【多模态大模型】Qwen2-VL解剖</a></p><p><a href="https://zhuanlan.zhihu.com/p/719388479">Qwen2-VL技术解析（二）- M-ROPE</a></p></blockquote><ul><li>引入<b>原生动态分辨率（Naive Dynamic Resolution, NDR）</b>机制 —— 实现了对<u>任意分辨率图像&#x2F;视频</u>的灵活处理。</li><li>设计<b>多模态旋转位置嵌入（Multimodal Rotary Position Embedding, M-RoPE）</b> —— 实现更有效的跨模态信息融合；还增强了长序列任务中的推理表现，在视频内容理解中尤为突出。</li><li>统一的图像和视频理解框架 —— 图像被处理为两个相同帧，保持与视频处理的一致性；使用 <u>3D tubes</u> 替代 <u>2D patches</u> 处理方式确保模型对多模态任务的全面适配。</li></ul><hr><ul><li><p>更强的复杂推理和决策的能力，可根据视觉环境和文字指令进行自动操作手机、机器人等设备。</p></li><li><p>支持除英语和中文外，也支持大多数欧洲语言、日语、韩语、阿拉伯语、越南语等<b>多语言</b>。</p></li><li><p>显著性能提升 —— 在多个权威基准数据集上实现了新 SoTA 。</p></li></ul><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p><img src="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/qwen2_vl_frame.jpg"></p><ul><li><p>视觉编码器：<code>Qwen2VisionTransformerPretrainedModel</code></p><p>图像和视频帧经过基于 ViT 的视觉编码器后，生成一系列视觉特征 token 。取代传统绝对位置编码，模型通过二维 RoPE 嵌入捕捉图像中<b>像素空间分布</b>信息。为了降低计算复杂度，视觉特征通过<u>一个简单的 MLP 层</u>进一步压缩，将邻近的 2x2 视觉 token 合并为一个 token ，同时在 token 序列的起始和结尾处分别添加<code>&lt;|vision_start|&gt;</code> 和 <code>&lt;|vision_end|&gt;</code> 标记。</p></li><li><p>语言模型：<code>Qwen2VLModel</code></p><p>使用 Qwen2-LM ，Qwen2 系列语言模型都是基于 Transformer 的解码器结构。视觉特征 tokens 和文本 tokens 通过统一的输入序列被送入 Qwen2 系列语言模型中，进行多模态信息的融合和处理。</p><blockquote><p>没有明显的 “视觉—语言” 适配器部分，视觉编码向量没有经过太多的处理直接进入了语言模型。</p><p>精致的 “视觉—语言” 适配器似乎正变得没那么重要，早在多模态模型诞生之初就有 ViLT 这样的将图片直接用线性层投影作为 Transformer 输入的架构，现在也有一些轻量的多模态模型用 MLP 对图片进行处理，直接删除了视觉编码器，可见多模态信息的融合不一定需要太复杂的结构。</p></blockquote></li><li><p>多模态旋转位置嵌入（M-RoPE）</p><blockquote><p>参考网页：<a href="https://zhuanlan.zhihu.com/p/719388479">https://zhuanlan.zhihu.com/p/719388479</a></p></blockquote><p>M-RoPE 在视觉和文本 token 之间建立精确的位置信息关联。特别是在视频任务中， M-RoPE 能够捕捉时间维度的动态变化，使模型能够处理长时间的视频内容。</p><p>对于文本，位置编码与传统的 1D-RoPE 一致，采用<u>一维序列编号</u>来标记词位位置。</p><p>对于图像，M-RoPE 通过<u>二维位置编码</u>为每个视觉 token 分配宽度和高度的位置标识，以准确表示图像中的空间结构。同时，图像中的时间维度编码值保持固定，因为静态图像没有时间变化。</p><p>对于视频，M-RoPE 进一步扩展，通过对<b>每一帧引入递增的时间位置标识</b>，并结合图像的宽度和高度位置编码，使模型能够理解视频帧的动态时间序列信息。</p></li></ul><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：模型根据输入图像的分辨率，动态调整视觉 token 的数量，而不是将所有图像固定调整到同一尺寸。</p><p>视频输入：对于视频数据，为了一致性，每个图像被视为两个相同的帧。为了平衡长视频处理的计算需求和整体训练效率，动态调整每个视频帧的分辨率，将每个视频的令牌总数限制在 16384 个，并利用动态分辨率机制压缩多帧内容为适合语言模型处理的视觉 token 序列。</p><p>模型输出：</p><ul><li>视觉问答（VQA）：回答与图像或视频相关的问题。</li><li>文档理解与 OCR ：识别和解析复杂文档中的文本信息。</li><li>视频内容分析：对长视频内容进行总结、生成对话内容或回答基于视频的问题。</li><li>代理能力：支持设备操作，如根据屏幕截图导航手机界面。</li></ul><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：ViT-训练，冻结语言模型参数"><a href="#第一阶段：ViT-训练，冻结语言模型参数" class="headerlink" title="第一阶段：ViT 训练，冻结语言模型参数"></a>第一阶段：ViT 训练，冻结语言模型参数</h3><p>使用大量的 “图像—文本” 对数据集来<u>增强大语言模型（LLM）中的语义理解</u>，让模型学会图像和文本之间的关系，以及通过 OCR 在图像中识别文本内容和图像分类任务。</p><p>在这一阶段，Qwen2-VL 大约使用 6000 亿个标记的语料库预训练。LLM 部分使用 Qwen2 的参数进行初始化，而 ViT 部分则使用<u>从 DFN 派生的 ViT</u> 进行初始化，同时将固定位置编码替换为 RoPE-2D 。</p><blockquote><p>DFN 即 <strong>Data Filtering Network</strong>，是一个<strong>小型的、专门用于 “筛数据” 的神经网络</strong>。它通常会接收<strong>图像—文本对</strong>，并预测其质量评分，高分样本被保留用于训练大型预训练模型。它本身不直接生成训练模型，而是在 “构建训练集” 的中间环节发挥作用。</p></blockquote><h3 id="第二阶段：解冻-ViT-和语言模型进行全参数训练"><a href="#第二阶段：解冻-ViT-和语言模型进行全参数训练" class="headerlink" title="第二阶段：解冻 ViT 和语言模型进行全参数训练"></a>第二阶段：解冻 ViT 和语言模型进行全参数训练</h3><p>解冻所有参数，并使用更广泛的数据进行训练，以实现更全面的学习。这个阶段引入了额外的 8000 亿个图像相关数据的标记，通过引入更多的混合 “图像—文本” 内容（混合图文内容、视觉问答数据集、多任务数据集和纯文本数据），以促进视觉和文本信息之间更细微的理解。仅对文本 tokens 提供监督。</p><h3 id="第三阶段：LLM-的微调"><a href="#第三阶段：LLM-的微调" class="headerlink" title="第三阶段：LLM 的微调"></a>第三阶段：LLM 的微调</h3><p>使用 ChatML 格式构建指令跟随数据。冻结 ViT 参数，使用指令数据集（<b>纯文本对话</b>和<b>多模态混合数据集</b>：图像问答、文档解析、多图比较、视频理解、视频流对话和基于代理的交互）对 LLM 进行专门的微调。</p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记_第1篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第一代-Qwen-VL-2023-08-2023-10"><a href="#第一代-Qwen-VL-2023-08-2023-10" class="headerlink" title="第一代 Qwen-VL 2023.08-2023.10"></a>第一代 Qwen-VL 2023.08-2023.10</h1><blockquote><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/689914137">Qwen-VL看这一篇就够了</a></p></blockquote><p>当时大多数的 LVLMs 都是以<b>粗粒度</b>的方式感知图像，缺乏<u>图像细粒度感知</u>的能力（包括<b>目标定位</b>和<b>文本读取</b>等）。基于当时的问题，Qwen 团队引入了一个<u>新的视觉编码器</u>和<u>位置感知适配器</u>，并且设计了一个三阶段训练的流程用于优化 Qwen-VL 模型。Qwen-VL 的特点：性能领先、支持多语言、<strong>支持任意交错的 “图像-文本” 数据</strong>、<strong>细粒度的视觉理解</strong>（例如 OCR）。Qwen-VL 相较于之前的<b>图文多模态大模型</b>多了一个功能：视觉定位，就是可以<b>给出一个框</b>将你想要的地方框出来。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/qwenvl_01.png"></p><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p>Qwen-VL 整个模型参数大致在 1.9B + 0.08B + 7.7B &#x3D; 9.6B 的参数数量。</p><ol><li><p>语言模型：Qwen-7B 大语言模型；</p></li><li><p>视觉编码器：ViT 的架构，参数量在 1.9B ，并且从<a href="https://github.com/mlfoundations/open_clip">开源项目 openclip 的 ViT-bigG</a> 权重开始初始化，训练和推理的过程中图像会被调整到特定的分辨率，也就是拆成 14x14 像素的 patch 块；</p></li><li><p>（位置级）视觉语言适配器：一个<u>随机权重初始化</u>的<u>单层交叉注意力模块</u>组成，参数量在 0.08B 。</p><p>该模块使用一组<b>可训练的向量（意思就是在训练中张量数值会改变，且梯度会流向这个向量）</b>作为 query 向量，将<b>视觉编码器的特征</b>作为 key 进行交叉注意力操作，将图像特征压缩到 256 长度的序列。并且将 2D 绝对位置编码用在交叉注意力机制中，以减轻压缩过程中的位置细节丢失。</p></li></ol><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：<code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code> 标记图像的开始和结束。图片通过<u>视觉编码器</u>和<u>（位置级）视觉语言适配器</u>模块，得到一个定长的特征序列。为了和文字输入区别，图片特征前后分别加上 <code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code>。</p><p>边界框输出：将边界框的值归一化在 <code>[0,1000)</code> 之间，并转换成特定的字符串格式 <code>&quot;(X_top_left, Y_top_left), (X_bottom_right, Y_bottom_right)&quot;</code> ，<code>&lt;box&gt;</code> 和 <code>&lt;/box&gt;</code> 分别添加在边界框字符串的开头和结尾。</p><p>内容输出：<code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code> 标记边界框所引用的内容。</p><blockquote><p>例如，某个任务的提示词：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">&lt;<span class="hljs-selector-tag">img</span>&gt;coyo700m/<span class="hljs-number">1</span><span class="hljs-selector-class">.jpg</span>&lt;/<span class="hljs-selector-tag">img</span>&gt;Generate the <span class="hljs-selector-tag">caption</span> in English with grounding:<br></code></pre></td></tr></table></figure><p>Qwen-VL 的回答如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Beautiful</span> shot of &lt;ref&gt;bees&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">661</span>,<span class="hljs-number">612</span>),(<span class="hljs-number">833</span>,<span class="hljs-number">812</span>)&lt;/box&gt;&lt;box&gt;(<span class="hljs-number">120</span>,<span class="hljs-number">555</span>),(<span class="hljs-number">265</span>,<span class="hljs-number">770</span>)&lt;/box&gt; gathering nectars from &lt;ref&gt;an apricot flower&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">224</span>,<span class="hljs-number">13</span>),(<span class="hljs-number">399</span>,<span class="hljs-number">313</span>) &lt;/box&gt;&lt;eos&gt;<br></code></pre></td></tr></table></figure></blockquote><p>模型处理视觉信息的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 确保只在第一次 forward 时处理视觉信息（past_key_values is None 表示不是缓存推理时）</span><br><span class="hljs-comment"># 图像 token 是以特殊的 image_start_id 开始，检测是否存在</span><br><span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> torch.<span class="hljs-built_in">any</span>(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>]):<br>    <br>    <span class="hljs-comment"># 找到图像 token 的边界位置</span><br>        bos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>])<br>        eos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 保证每个起始标记都在一个样本内部结束</span><br>        <span class="hljs-keyword">assert</span> (bos_pos[<span class="hljs-number">0</span>] == eos_pos[<span class="hljs-number">0</span>]).<span class="hljs-built_in">all</span>()<br>        <br>        <span class="hljs-comment"># 构建 img_pos：(batch_idx, start_idx, end_idx)</span><br>        img_pos = torch.stack((bos_pos[<span class="hljs-number">0</span>], bos_pos[<span class="hljs-number">1</span>], eos_pos[<span class="hljs-number">1</span>]), dim=<span class="hljs-number">1</span>)<br>        images = []<br>        <span class="hljs-keyword">for</span> i, a, b <span class="hljs-keyword">in</span> img_pos:<br>            <br>            <span class="hljs-comment"># 截取 patch token（跳过起始和终止标志）</span><br>            image = input_ids[i][a + <span class="hljs-number">1</span> : b - <span class="hljs-number">1</span>].tolist()<br>            <span class="hljs-comment"># 截取图片结束 token</span><br>            image = image[ : image.index(<span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">2</span>)]<br>            <span class="hljs-comment"># 解码为 utf-8 图像数据（说明 image token 实际是原始图片字节的编码）</span><br>            images.append(<span class="hljs-built_in">bytes</span>(image).decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>            <br>            <span class="hljs-comment"># 调用视觉编码器</span><br>            images = <span class="hljs-variable language_">self</span>.visual.encode(images)<br>            <span class="hljs-keyword">assert</span> images.shape[<span class="hljs-number">0</span>] == <span class="hljs-built_in">len</span>(images)<br>            <br>            fake_images = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">if</span> fake_images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                hidden_states = hidden_states + images.mean()*<span class="hljs-number">0</span><br>            <br><span class="hljs-comment"># 将图像嵌入写入对应位置 a+1 : b 的 hidden state（对应 patch tokens）</span><br>            <span class="hljs-keyword">elif</span> images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">for</span> idx, (i, a, b) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(img_pos):<br>                    hidden_states[i][a + <span class="hljs-number">1</span> : b] = images[idx]<br></code></pre></td></tr></table></figure><blockquote><p>Q：<code>image = image[ : image.index(self.config.visual[&#39;image_start_id&#39;] + 2)]</code> 这段代码的作用？</p><p>A：因为模型在输入图片时，有时会预留更多 token 空间来填充图像信息，例如 padding 和 filter ，这就导致图像本身可能变长。这些填充的 token 确实是图像的一部分，但是输入到视觉编码器中产生干扰，因此需要额外再加一行代码对这些填充 token 做进一步过滤。</p></blockquote><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：预训练过程"><a href="#第一阶段：预训练过程" class="headerlink" title="第一阶段：预训练过程"></a>第一阶段：预训练过程</h3><p>使用互联网网页抓取的 ”图像—文本“ 对，50 亿条数据清洗后剩下 14 亿数据，其中 77.3% 为英文数据，22.7% 为中文数据。<b>这一阶段冻结语言模型</b>，训练视觉编码器和视觉语言适配器，输入图像调整为 224x224 的分辨率（按照每 14 像素分割后得到 16x16&#x3D;256 个 patch），batch size 为 30720 ，训练 50000 步，使用 15 亿数据。</p><h3 id="第二阶段：多任务预训练"><a href="#第二阶段：多任务预训练" class="headerlink" title="第二阶段：多任务预训练"></a>第二阶段：多任务预训练</h3><p>加入了高质量、细粒度的图像和文本数据，使用了更大的分辨率和交错的 ”图像—文本“ 数据。在 7 个任务上对 Qwen-VL 进行训练。将视觉编码器的分辨率从 224x224 增加到 448x448，以减少图像下采样造成的信息损失。<b>这一过程没有冻结任何模块。</b></p><h3 id="第三阶段：监督微调"><a href="#第三阶段：监督微调" class="headerlink" title="第三阶段：监督微调"></a>第三阶段：监督微调</h3><p>数据<u>来自 LLM 生成</u>的图像标注或对话数据，这些数据通常只处理<b>单图像对话和推理</b>，且仅限于图像内容理解。</p><p>通过<u>手动标注、模型生成和策略组合</u>，构建了一个额外的对话数据集，以将<b>定位和多图像理解能力</b>融入 Qwen-VL 模型中。在训练过程中混合了多模态和<u>纯文本对话数据</u>，以确保模型的对话能力具有普遍性。</p><p>指令微调数据量达到 35 万条。<b>这一过程冻结视觉编码器。</b></p><h2 id="模型代码应用"><a href="#模型代码应用" class="headerlink" title="模型代码应用"></a>模型代码应用</h2><h3 id="图片和文本的加载"><a href="#图片和文本的加载" class="headerlink" title="图片和文本的加载"></a>图片和文本的加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">query = tokenizer.from_list_format([<br>    &#123;<span class="hljs-string">&#x27;image&#x27;</span>: <span class="hljs-string">&#x27;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#x27;</span>&#125;,<br>    &#123;<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;这是什么&#x27;</span>&#125;,<br>])<br></code></pre></td></tr></table></figure><h3 id="图像到字符串的转换"><a href="#图像到字符串的转换" class="headerlink" title="图像到字符串的转换"></a>图像到字符串的转换</h3><p>Qwen-VL 将图片都处理成：“Picture 1”、“Picture 2”、“Picture 3” 等<u>字符串</u>格式，并添加上<u>图片的开始和结束 token</u> ，文本直接拼接，box 的 <u>ref 添加上开始结束符</u>拼接，box <u>坐标从数字整理成字符串格式</u>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_list_format</span>(<span class="hljs-params">self, list_format: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>]</span>):<br>    text = <span class="hljs-string">&#x27;&#x27;</span><br>    num_images = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> list_format:  <span class="hljs-comment"># 每个 ele 都是字典</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 图片处理成这样的字符串，再加上图像自身、开始和结束的 tokens</span><br>            num_images += <span class="hljs-number">1</span><br>            text += <span class="hljs-string">f&#x27;Picture <span class="hljs-subst">&#123;num_images&#125;</span>: &#x27;</span><br>            text += <span class="hljs-variable language_">self</span>.image_start_tag + ele[<span class="hljs-string">&#x27;image&#x27;</span>] + <span class="hljs-variable language_">self</span>.image_end_tag<br>            text += <span class="hljs-string">&#x27;\n&#x27;</span><br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是文本，直接添加文本</span><br>            text += ele[<span class="hljs-string">&#x27;text&#x27;</span>]<br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;box&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是定位框，先考虑有没有参考对象</span><br>            <span class="hljs-comment"># 如果有的话，先添加参考对象自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-comment"># 没有的话，添加定位框自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ref&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>                text += <span class="hljs-variable language_">self</span>.ref_start_tag + ele[<span class="hljs-string">&#x27;ref&#x27;</span>] + <span class="hljs-variable language_">self</span>.ref_end_tag<br>            <span class="hljs-keyword">for</span> box <span class="hljs-keyword">in</span> ele[<span class="hljs-string">&#x27;box&#x27;</span>]:<br>                text += <span class="hljs-variable language_">self</span>.box_start_tag + <span class="hljs-string">&#x27;(%d,%d),(%d,%d)&#x27;</span> % (box[<span class="hljs-number">0</span>], box[<span class="hljs-number">1</span>], box[<span class="hljs-number">2</span>], box[<span class="hljs-number">3</span>]) + <span class="hljs-variable language_">self</span>.box_end_tag<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Unsupport element: &quot;</span> + <span class="hljs-built_in">str</span>(ele))<br>    <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure><h3 id="图像的编码"><a href="#图像的编码" class="headerlink" title="图像的编码"></a>图像的编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, image_paths: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>):<br>        images = []<br>        <span class="hljs-keyword">for</span> image_path <span class="hljs-keyword">in</span> image_paths:<br>            <span class="hljs-keyword">if</span> image_path.startswith(<span class="hljs-string">&quot;http://&quot;</span>) <span class="hljs-keyword">or</span> image_path.startswith(<span class="hljs-string">&quot;https://&quot;</span>):<br>                image = Image.<span class="hljs-built_in">open</span>(requests.get(image_path, stream=<span class="hljs-literal">True</span>).raw)<br>            <span class="hljs-keyword">else</span>:<br>                image = Image.<span class="hljs-built_in">open</span>(image_path)<br>            image = image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>            images.append(<span class="hljs-variable language_">self</span>.image_transform(image))<br>        images = torch.stack(images, dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>(images)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: torch.Tensor</span>):<br>        x = x.to(<br>            dtype=<span class="hljs-variable language_">self</span>.transformer.get_cast_dtype(),<br>            device=<span class="hljs-variable language_">self</span>.transformer.get_cast_device(),<br>        )<br>        <span class="hljs-comment"># to patches</span><br>        x = <span class="hljs-variable language_">self</span>.conv1(x)  <span class="hljs-comment"># shape = [*, width, grid, grid]</span><br>        x = x.reshape(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, width, grid ** 2]</span><br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, grid ** 2, width]</span><br><br>        x = x + get_abs_pos(<span class="hljs-variable language_">self</span>.positional_embedding, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.ln_pre(x)<br><br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># NLD -&gt; LND</span><br>        x = <span class="hljs-variable language_">self</span>.transformer(x)<br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># LND -&gt; NLD</span><br><br>        x = <span class="hljs-variable language_">self</span>.attn_pool(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_post(x)<br>        x = x @ <span class="hljs-variable language_">self</span>.proj<br></code></pre></td></tr></table></figure><p>图片是经过 resize 和归一化后输入 ViT 进行编码，ViT 编码后经过交叉注意力机制、归一化然后投影到 embedding 维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.attn_pool = Resampler(<br>            grid_size=<span class="hljs-built_in">int</span>(math.sqrt(n_queries)),<br>            embed_dim=output_dim,<br>            num_heads=output_dim // <span class="hljs-number">128</span>,<br>            kv_dim=width,<br>            norm_layer=norm_layer,<br>        )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, attn_mask=<span class="hljs-literal">None</span></span>):<br><br>        pos_embed = get_abs_pos(<span class="hljs-variable language_">self</span>.pos_embed, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.kv_proj(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_kv(x).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br><br>        N = x.shape[<span class="hljs-number">1</span>]<br>        q = <span class="hljs-variable language_">self</span>.ln_q(<span class="hljs-variable language_">self</span>.query)<br>        out = <span class="hljs-variable language_">self</span>.attn(<br>            <span class="hljs-variable language_">self</span>._repeat(q, N) + <span class="hljs-variable language_">self</span>.pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x + pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x,<br>            attn_mask=attn_mask)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> out.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
