<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记-第2篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第二代-Qwen2-VL-2024-09-2024-10"><a href="#第二代-Qwen2-VL-2024-09-2024-10" class="headerlink" title="第二代 Qwen2-VL 2024.09-2024.10"></a>第二代 Qwen2-VL 2024.09-2024.10</h1><ul><li>引入<b>原生动态分辨率（Naive Dynamic Resolution, NDR）</b>机制 —— 实现了对<u>任意分辨率图像&#x2F;视频</u>的灵活处理。</li><li>设计<b>多模态旋转位置嵌入（Multimodal Rotary Position Embedding, M-RoPE）</b> —— 提升了模型的跨模态理解能力，还增强了长序列任务中的推理表现，在视频内容理解中尤为突出。</li><li>更强的复杂推理和决策的能力，可根据视觉环境和文字指令进行自动操作手机、机器人等设备。</li><li>支持除英语和中文外，也支持大多数欧洲语言、日语、韩语、阿拉伯语、越南语等<b>多语言</b>。</li><li>统一的多模态学习框架 —— 确保模型对多模态任务的全面适配。</li><li>显著性能提升 —— 在多个权威基准数据集上实现了新 SoTA 。</li></ul><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p><img src="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/qwen2_vl_frame.jpg"></p><ul><li><p>视觉编码器</p><p>图像和视频帧经过基于 ViT 的视觉编码器后，生成一系列视觉特征 token 。取代传统绝对位置编码，模型通过二维 RoPE 嵌入捕捉图像中<b>像素空间分布</b>信息。为了降低计算复杂度，视觉特征通过<u>一个简单的 MLP 层</u>进一步压缩，将邻近的 2x2 视觉 token 合并为一个 token ，同时在 token 序列的起始和结尾处分别添加<code>&lt;|vision_start|&gt;</code> 和 <code>&lt;|vision_end|&gt;</code> 标记。</p></li><li><p>语言模型</p><p>使用 Qwen2-LM ，Qwen2 系列语言模型都是基于 Transformer 的解码器结构。视觉特征 tokens 和文本 tokens 通过统一的输入序列被送入 Qwen2 系列语言模型中，进行多模态信息的融合和处理。</p></li><li><p>多模态旋转位置嵌入（M-RoPE）</p><p>M-RoPE 在视觉和文本 token 之间建立精确的位置信息关联。特别是在视频任务中， M-RoPE 能够捕捉时间维度的动态变化，使模型能够处理长时间的视频内容。</p><p>对于文本，位置编码与传统的 1D-RoPE 一致，采用<u>一维序列编号</u>来标记词位位置。</p><p>对于图像，M-RoPE 通过<u>二维位置编码</u>为每个视觉 token 分配宽度和高度的位置标识，以准确表示图像中的空间结构。同时，图像中的时间维度编码值保持固定，因为静态图像没有时间变化。</p><p>对于视频，M-RoPE 进一步扩展，通过对<b>每一帧引入递增的时间位置标识</b>，并结合图像的宽度和高度位置编码，使模型能够理解视频帧的动态时间序列信息。</p></li></ul><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：模型根据输入图像的分辨率，动态调整视觉 token 的数量，而不是将所有图像固定调整到同一尺寸。</p><p>视频输入：对于视频数据，为了一致性，每个图像被视为两个相同的帧。为了平衡长视频处理的计算需求和整体训练效率，动态调整每个视频帧的分辨率，将每个视频的令牌总数限制在 16384 个，并利用动态分辨率机制压缩多帧内容为适合语言模型处理的视觉 token 序列。</p><p>模型输出：</p><ul><li>视觉问答（VQA）：回答与图像或视频相关的问题。</li><li>文档理解与 OCR ：识别和解析复杂文档中的文本信息。</li><li>视频内容分析：对长视频内容进行总结、生成对话内容或回答基于视频的问题。</li><li>代理能力：支持设备操作，如根据屏幕截图导航手机界面。</li></ul><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：ViT-训练，冻结语言模型参数"><a href="#第一阶段：ViT-训练，冻结语言模型参数" class="headerlink" title="第一阶段：ViT 训练，冻结语言模型参数"></a>第一阶段：ViT 训练，冻结语言模型参数</h3><p>使用大量的 “图像—文本” 对数据集来增强大语言模型（LLM）中的语义理解，让模型学会图像和文本之间的关系，以及通过 OCR 在图像中识别文本内容和图像分类任务。</p><p>在这一阶段，Qwen2-VL 大约使用 6000 亿个标记的语料库预训练。LLM 部分使用 Qwen2 的参数进行初始化，而 ViT 部分则使用<u>从 DFN 派生的 ViT</u> 进行初始化。</p><blockquote><p>DFN 即 <strong>Data Filtering Network</strong>，是一个<strong>小型的、专门用于 “筛数据” 的神经网络</strong>。它通常会接收<strong>图像—文本对</strong>，并预测其质量评分，高分样本被保留用于训练大型预训练模型。它本身不直接生成训练模型，而是在 “构建训练集” 的中间环节发挥作用。</p></blockquote><h3 id="第二阶段：解冻-ViT-和语言模型进行全参数训练"><a href="#第二阶段：解冻-ViT-和语言模型进行全参数训练" class="headerlink" title="第二阶段：解冻 ViT 和语言模型进行全参数训练"></a>第二阶段：解冻 ViT 和语言模型进行全参数训练</h3><p>解冻所有参数，并使用更广泛的数据进行训练，以实现更全面的学习。这个阶段引入了额外的 8000 亿个图像相关数据的标记，通过引入更多的混合 “图像—文本” 内容，以促进视觉和文本信息之间更细微的理解。</p><h3 id="第三阶段：LLM-的微调"><a href="#第三阶段：LLM-的微调" class="headerlink" title="第三阶段：LLM 的微调"></a>第三阶段：LLM 的微调</h3><p>使用 ChatML 格式构建指令跟随数据。冻结 ViT 参数，使用指令数据集对 LLM 进行专门的微调。</p><h3 id="训练手段"><a href="#训练手段" class="headerlink" title="训练手段"></a>训练手段</h3><ol><li>数据预处理（data preprocessing）：先将不同分辨率图片进行 patch 处理</li><li>连续型词元丢弃（continuous token dropping）：过去，Token Dropout 也就是<b>训练过程中随机省略输入 patch </b>，是以固定比例进行的，这在处理<u>相同分辨率的图像</u>时效果良好。但在支持任意分辨率的情况下，需要根据图像的大小来动态调整 Token Dropout 的比例。简单来说，大小为 10x30 的图像不能与 100x300 的图像使用相同的丢弃比例。通过根据图像尺寸进行调整，可以确保不同分辨率下的训练效果最佳</li><li>分辨率采样。在传统的 ViT 模型中，训练时存在性能与效率的权衡：较小分辨率的图像能<u>提高训练效率</u>，而较大分辨率的图像<u>有助于提高模型在评估阶段的表现</u>。因此，模型通常会先在较低分辨率下预训练，然后再在较高分辨率下进行微调。相比之下，NaViT 具有更大的灵活性，它通过从不同的图像尺寸中进行采样，支持混合分辨率训练，并保持每张图像的原始纵横比。这种方法不仅提高了训练效率，还能让模型接触到更大分辨率的图像，从而在相同模型大小和训练时长下，显著提升性能。</li></ol><h2 id="模型代码应用"><a href="#模型代码应用" class="headerlink" title="模型代码应用"></a>模型代码应用</h2><h2 id="引申：从-Pix2Struct-到-NaViT-到-NDR"><a href="#引申：从-Pix2Struct-到-NaViT-到-NDR" class="headerlink" title="引申：从 Pix2Struct 到 NaViT 到 NDR"></a>引申：从 Pix2Struct 到 NaViT 到 NDR</h2><p>NDR 这一方法的引入来自 2024 年 NIPS 的一个工作：NaViT。</p><p>原生 ViT 对图片的处理是先将图片 resize 到固定尺寸，然后分为 nxn 个 patch，每个 patch 得到 patch embedding，再输入 transformer 层中。但这种情况下对于非固定尺寸的照片会扭曲其内容。</p><p>Pix2Struct 针对 ViT 需要 resize 到固定尺寸的缺点，提出了一种方法保持长宽比率的缩放，并且为此构建了一个可学习的二维位置编码：<code>max_seq * max_seq * embed_dim</code> 的矩阵，Pix2Struct 引入了可学习的二维绝对位置编码。这些编码的尺寸为 <code>[maxLen, maxLen]</code> ，并且通过每个 patch 的  (x, y)  坐标进行索引。这意味着模型能够根据图像中的坐标来学习特定的空间位置信息，从而在面对不同的图像比例时表现得更加灵活和精准。它允许模型处理的图像分辨率最高可以达到 <code>R = P·maxLen</code> ， P 表示 patch 的数量，maxLen 代表最大长度。然而，这种方法也有其局限性，那就是训练过程中必须看到所有可能的 <code>(x, y)</code> 坐标组合，才能让模型充分学习到位置信息。</p><p>NaViT 对此进行了以下做法：</p><ol><li><p>模型层面：</p><p>Masked Self-Attention。NaViT 模型引入了 Masked Self-Attention 机制，确保在进行自注意力计算时，每个图像只能关注到自身内容，而不会与其他图像发生交互；</p><p>Masked Pooling：每张图片上各自去进行 pooling ，独立的进行信息聚合，而不是全局聚合，有助于保留图像的局部特征；</p></li><li><p>位置编码：为了支持可变的长宽比并能够将位置编码扩展到未见过的分辨率，研究者提出了因子化的位置编码方法， 将位置编码分解为 x 和 y 坐标的独立编码 $\phi(x)$ 和 $\phi(y)$ ，最后将它们相加。</p><p>在此基础上，提出了两种编码方案：</p><p>（1）绝对位置编码：其中 $\phi(p)$ 是根据 patch 的绝对位置索引的函数，即位置编码是基于图像中的绝对坐标进行计算的；</p><p>（2）分数位置编码：其中 $\phi(r)$ 是相对距离的函数，即相对于 patch 边长的距离，这种编码独立于图像尺寸，从而部分模糊了图像的原始长宽比，优势在于它能够提供独立于图像尺寸的位置编码参数，但也因此部分掩盖了原始长宽比的信息，仅在 patch 数量中隐含地体现了这一信息。</p></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记_第1篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第一代-Qwen-VL-2023-08-2023-10"><a href="#第一代-Qwen-VL-2023-08-2023-10" class="headerlink" title="第一代 Qwen-VL 2023.08-2023.10"></a>第一代 Qwen-VL 2023.08-2023.10</h1><p>当时大多数的 LVLMs 都是以<b>粗粒度</b>的方式感知图像，缺乏<u>图像细粒度感知</u>的能力（包括<b>目标定位</b>和<b>文本读取</b>等）。基于当时的问题，Qwen 团队引入了一个<u>新的视觉编码器</u>和<u>位置感知适配器</u>，并且设计了一个三阶段训练的流程用于优化 Qwen-VL 模型。Qwen-VL 的特点：性能领先、支持多语言、<strong>支持任意交错的 “图像-文本” 数据</strong>、<strong>细粒度的视觉理解</strong>（例如 OCR）。Qwen-VL 相较于之前的<b>图文多模态大模型</b>多了一个功能：视觉定位，就是可以<b>给出一个框</b>将你想要的地方框出来。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/qwenvl_01.png"></p><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p>Qwen-VL 整个模型参数大致在 1.9B + 0.08B + 7.7B &#x3D; 9.6B 的参数数量。</p><ol><li><p>语言模型：Qwen-7B 大语言模型；</p></li><li><p>视觉编码器：ViT 的架构，参数量在 1.9B ，并且从<a href="https://github.com/mlfoundations/open_clip">开源项目 openclip 的 ViT-bigG</a> 权重开始初始化，训练和推理的过程中图像会被调整到特定的分辨率，也就是拆成 14x14 像素的 patch 块；</p></li><li><p>（位置级）视觉语言适配器：一个<u>随机权重初始化</u>的<u>单层交叉注意力模块</u>组成，参数量在 0.08B 。</p><p>该模块使用一组<b>可训练的向量（意思就是在训练中张量数值会改变，且梯度会流向这个向量）</b>作为 query 向量，将<b>视觉编码器的特征</b>作为 key 进行交叉注意力操作，将图像特征压缩到 256 长度的序列。并且将 2D 绝对位置编码用在交叉注意力机制中，以减轻压缩过程中的位置细节丢失。</p></li></ol><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：<code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code> 标记图像的开始和结束。图片通过<u>视觉编码器</u>和<u>（位置级）视觉语言适配器</u>模块，得到一个定长的特征序列。为了和文字输入区别，图片特征前后分别加上 <code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code>。</p><p>边界框输出：将边界框的值归一化在 <code>[0,1000)</code> 之间，并转换成特定的字符串格式 <code>&quot;(X_top_left, Y_top_left), (X_bottom_right, Y_bottom_right)&quot;</code> ，<code>&lt;box&gt;</code> 和 <code>&lt;/box&gt;</code> 分别添加在边界框字符串的开头和结尾。</p><p>内容输出：<code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code> 标记边界框所引用的内容。</p><blockquote><p>例如，某个任务的提示词：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">&lt;<span class="hljs-selector-tag">img</span>&gt;coyo700m/<span class="hljs-number">1</span><span class="hljs-selector-class">.jpg</span>&lt;/<span class="hljs-selector-tag">img</span>&gt;Generate the <span class="hljs-selector-tag">caption</span> in English with grounding:<br></code></pre></td></tr></table></figure><p>Qwen-VL 的回答如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Beautiful</span> shot of &lt;ref&gt;bees&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">661</span>,<span class="hljs-number">612</span>),(<span class="hljs-number">833</span>,<span class="hljs-number">812</span>)&lt;/box&gt;&lt;box&gt;(<span class="hljs-number">120</span>,<span class="hljs-number">555</span>),(<span class="hljs-number">265</span>,<span class="hljs-number">770</span>)&lt;/box&gt; gathering nectars from &lt;ref&gt;an apricot flower&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">224</span>,<span class="hljs-number">13</span>),(<span class="hljs-number">399</span>,<span class="hljs-number">313</span>) &lt;/box&gt;&lt;eos&gt;<br></code></pre></td></tr></table></figure></blockquote><p>模型处理视觉信息的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 确保只在第一次 forward 时处理视觉信息（past_key_values is None 表示不是缓存推理时）</span><br><span class="hljs-comment"># 图像 token 是以特殊的 image_start_id 开始，检测是否存在</span><br><span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> torch.<span class="hljs-built_in">any</span>(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>]):<br>    <br>    <span class="hljs-comment"># 找到图像 token 的边界位置</span><br>        bos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>])<br>        eos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 保证每个起始标记都在一个样本内部结束</span><br>        <span class="hljs-keyword">assert</span> (bos_pos[<span class="hljs-number">0</span>] == eos_pos[<span class="hljs-number">0</span>]).<span class="hljs-built_in">all</span>()<br>        <br>        <span class="hljs-comment"># 构建 img_pos：(batch_idx, start_idx, end_idx)</span><br>        img_pos = torch.stack((bos_pos[<span class="hljs-number">0</span>], bos_pos[<span class="hljs-number">1</span>], eos_pos[<span class="hljs-number">1</span>]), dim=<span class="hljs-number">1</span>)<br>        images = []<br>        <span class="hljs-keyword">for</span> i, a, b <span class="hljs-keyword">in</span> img_pos:<br>            <br>            <span class="hljs-comment"># 截取 patch token（跳过起始和终止标志）</span><br>            image = input_ids[i][a + <span class="hljs-number">1</span> : b - <span class="hljs-number">1</span>].tolist()<br>            <span class="hljs-comment"># 截取图片结束 token</span><br>            image = image[ : image.index(<span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">2</span>)]<br>            <span class="hljs-comment"># 解码为 utf-8 图像数据（说明 image token 实际是原始图片字节的编码）</span><br>            images.append(<span class="hljs-built_in">bytes</span>(image).decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>            <br>            <span class="hljs-comment"># 调用视觉编码器</span><br>            images = <span class="hljs-variable language_">self</span>.visual.encode(images)<br>            <span class="hljs-keyword">assert</span> images.shape[<span class="hljs-number">0</span>] == <span class="hljs-built_in">len</span>(images)<br>            <br>            fake_images = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">if</span> fake_images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                hidden_states = hidden_states + images.mean()*<span class="hljs-number">0</span><br>            <br><span class="hljs-comment"># 将图像嵌入写入对应位置 a+1 : b 的 hidden state（对应 patch tokens）</span><br>            <span class="hljs-keyword">elif</span> images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">for</span> idx, (i, a, b) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(img_pos):<br>                    hidden_states[i][a + <span class="hljs-number">1</span> : b] = images[idx]<br></code></pre></td></tr></table></figure><blockquote><p>Q：<code>image = image[ : image.index(self.config.visual[&#39;image_start_id&#39;] + 2)]</code> 这段代码的作用？</p><p>A：因为模型在输入图片时，有时会预留更多 token 空间来填充图像信息，例如 padding 和 filter ，这就导致图像本身可能变长。这些填充的 token 确实是图像的一部分，但是输入到视觉编码器中产生干扰，因此需要额外再加一行代码对这些填充 token 做进一步过滤。</p></blockquote><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：预训练过程"><a href="#第一阶段：预训练过程" class="headerlink" title="第一阶段：预训练过程"></a>第一阶段：预训练过程</h3><p>使用互联网网页抓取的 ”图像—文本“ 对，50 亿条数据清洗后剩下 14 亿数据，其中 77.3% 为英文数据，22.7% 为中文数据。<b>这一阶段冻结语言模型</b>，训练视觉编码器和视觉语言适配器，输入图像调整为 224x224 的分辨率（按照每 14 像素分割后得到 16x16&#x3D;256 个 patch），batch size 为 30720 ，训练 50000 步，使用 15 亿数据。</p><h3 id="第二阶段：多任务预训练"><a href="#第二阶段：多任务预训练" class="headerlink" title="第二阶段：多任务预训练"></a>第二阶段：多任务预训练</h3><p>加入了高质量、细粒度的图像和文本数据，使用了更大的分辨率和交错的 ”图像—文本“ 数据。在 7 个任务上对 Qwen-VL 进行训练。将视觉编码器的分辨率从 224x224 增加到 448x448，以减少图像下采样造成的信息损失。<b>这一过程没有冻结任何模块。</b></p><h3 id="第三阶段：监督微调"><a href="#第三阶段：监督微调" class="headerlink" title="第三阶段：监督微调"></a>第三阶段：监督微调</h3><p>数据<u>来自 LLM 生成</u>的图像标注或对话数据，这些数据通常只处理<b>单图像对话和推理</b>，且仅限于图像内容理解。</p><p>通过<u>手动标注、模型生成和策略组合</u>，构建了一个额外的对话数据集，以将<b>定位和多图像理解能力</b>融入 Qwen-VL 模型中。在训练过程中混合了多模态和<u>纯文本对话数据</u>，以确保模型的对话能力具有普遍性。</p><p>指令微调数据量达到 35 万条。<b>这一过程冻结视觉编码器。</b></p><h2 id="模型代码应用"><a href="#模型代码应用" class="headerlink" title="模型代码应用"></a>模型代码应用</h2><h3 id="图片和文本的加载"><a href="#图片和文本的加载" class="headerlink" title="图片和文本的加载"></a>图片和文本的加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">query = tokenizer.from_list_format([<br>    &#123;<span class="hljs-string">&#x27;image&#x27;</span>: <span class="hljs-string">&#x27;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#x27;</span>&#125;,<br>    &#123;<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;这是什么&#x27;</span>&#125;,<br>])<br></code></pre></td></tr></table></figure><h3 id="图像到字符串的转换"><a href="#图像到字符串的转换" class="headerlink" title="图像到字符串的转换"></a>图像到字符串的转换</h3><p>Qwen-VL 将图片都处理成：“Picture 1”、“Picture 2”、“Picture 3” 等<u>字符串</u>格式，并添加上<u>图片的开始和结束 token</u> ，文本直接拼接，box 的 <u>ref 添加上开始结束符</u>拼接，box <u>坐标从数字整理成字符串格式</u>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_list_format</span>(<span class="hljs-params">self, list_format: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>]</span>):<br>    text = <span class="hljs-string">&#x27;&#x27;</span><br>    num_images = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> list_format:  <span class="hljs-comment"># 每个 ele 都是字典</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 图片处理成这样的字符串，再加上图像自身、开始和结束的 tokens</span><br>            num_images += <span class="hljs-number">1</span><br>            text += <span class="hljs-string">f&#x27;Picture <span class="hljs-subst">&#123;num_images&#125;</span>: &#x27;</span><br>            text += <span class="hljs-variable language_">self</span>.image_start_tag + ele[<span class="hljs-string">&#x27;image&#x27;</span>] + <span class="hljs-variable language_">self</span>.image_end_tag<br>            text += <span class="hljs-string">&#x27;\n&#x27;</span><br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是文本，直接添加文本</span><br>            text += ele[<span class="hljs-string">&#x27;text&#x27;</span>]<br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;box&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是定位框，先考虑有没有参考对象</span><br>            <span class="hljs-comment"># 如果有的话，先添加参考对象自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-comment"># 没有的话，添加定位框自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ref&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>                text += <span class="hljs-variable language_">self</span>.ref_start_tag + ele[<span class="hljs-string">&#x27;ref&#x27;</span>] + <span class="hljs-variable language_">self</span>.ref_end_tag<br>            <span class="hljs-keyword">for</span> box <span class="hljs-keyword">in</span> ele[<span class="hljs-string">&#x27;box&#x27;</span>]:<br>                text += <span class="hljs-variable language_">self</span>.box_start_tag + <span class="hljs-string">&#x27;(%d,%d),(%d,%d)&#x27;</span> % (box[<span class="hljs-number">0</span>], box[<span class="hljs-number">1</span>], box[<span class="hljs-number">2</span>], box[<span class="hljs-number">3</span>]) + <span class="hljs-variable language_">self</span>.box_end_tag<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Unsupport element: &quot;</span> + <span class="hljs-built_in">str</span>(ele))<br>    <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure><h3 id="图像的编码"><a href="#图像的编码" class="headerlink" title="图像的编码"></a>图像的编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, image_paths: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>):<br>        images = []<br>        <span class="hljs-keyword">for</span> image_path <span class="hljs-keyword">in</span> image_paths:<br>            <span class="hljs-keyword">if</span> image_path.startswith(<span class="hljs-string">&quot;http://&quot;</span>) <span class="hljs-keyword">or</span> image_path.startswith(<span class="hljs-string">&quot;https://&quot;</span>):<br>                image = Image.<span class="hljs-built_in">open</span>(requests.get(image_path, stream=<span class="hljs-literal">True</span>).raw)<br>            <span class="hljs-keyword">else</span>:<br>                image = Image.<span class="hljs-built_in">open</span>(image_path)<br>            image = image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>            images.append(<span class="hljs-variable language_">self</span>.image_transform(image))<br>        images = torch.stack(images, dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>(images)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: torch.Tensor</span>):<br>        x = x.to(<br>            dtype=<span class="hljs-variable language_">self</span>.transformer.get_cast_dtype(),<br>            device=<span class="hljs-variable language_">self</span>.transformer.get_cast_device(),<br>        )<br>        <span class="hljs-comment"># to patches</span><br>        x = <span class="hljs-variable language_">self</span>.conv1(x)  <span class="hljs-comment"># shape = [*, width, grid, grid]</span><br>        x = x.reshape(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, width, grid ** 2]</span><br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, grid ** 2, width]</span><br><br>        x = x + get_abs_pos(<span class="hljs-variable language_">self</span>.positional_embedding, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.ln_pre(x)<br><br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># NLD -&gt; LND</span><br>        x = <span class="hljs-variable language_">self</span>.transformer(x)<br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># LND -&gt; NLD</span><br><br>        x = <span class="hljs-variable language_">self</span>.attn_pool(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_post(x)<br>        x = x @ <span class="hljs-variable language_">self</span>.proj<br></code></pre></td></tr></table></figure><p>图片是经过 resize 和归一化后输入 ViT 进行编码，ViT 编码后经过交叉注意力机制、归一化然后投影到 embedding 维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.attn_pool = Resampler(<br>            grid_size=<span class="hljs-built_in">int</span>(math.sqrt(n_queries)),<br>            embed_dim=output_dim,<br>            num_heads=output_dim // <span class="hljs-number">128</span>,<br>            kv_dim=width,<br>            norm_layer=norm_layer,<br>        )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, attn_mask=<span class="hljs-literal">None</span></span>):<br><br>        pos_embed = get_abs_pos(<span class="hljs-variable language_">self</span>.pos_embed, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.kv_proj(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_kv(x).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br><br>        N = x.shape[<span class="hljs-number">1</span>]<br>        q = <span class="hljs-variable language_">self</span>.ln_q(<span class="hljs-variable language_">self</span>.query)<br>        out = <span class="hljs-variable language_">self</span>.attn(<br>            <span class="hljs-variable language_">self</span>._repeat(q, N) + <span class="hljs-variable language_">self</span>.pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x + pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x,<br>            attn_mask=attn_mask)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> out.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
