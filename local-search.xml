<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记-第3篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC3%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC3%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第二代-Qwen2-VL-应用案例"><a href="#第二代-Qwen2-VL-应用案例" class="headerlink" title="第二代 Qwen2-VL 应用案例"></a>第二代 Qwen2-VL 应用案例</h1><p>[TOC]</p><h2 id="模型下载-——-基于魔塔社区"><a href="#模型下载-——-基于魔塔社区" class="headerlink" title="模型下载 —— 基于魔塔社区"></a>模型下载 —— 基于魔塔社区</h2><p>本次 Qwen2-VL 开源了两个尺寸的模型，<strong>Qwen2-VL-2B-Instruct</strong> 和 <strong>Qwen2-VL-7B-Instruct</strong>，以及其 GPTQ 和 AWQ 的量化版本。</p><p><strong>模型链接：</strong></p><p>Qwen2-VL-2B-Instruct：<a href="https://www.modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct">https://www.modelscope.cn/models/qwen/Qwen2-VL-2B-Instruct</a></p><p>Qwen2-VL-7B-Instruct：<a href="https://www.modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct">https://www.modelscope.cn/models/qwen/Qwen2-VL-7B-Instruct</a></p><p>推荐使用 ModelScope CLI 下载模型</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">modelscope download --model=qwen/Qwen2-VL-7B-Instruct --local_dir ./Qwen2-VL-7B-Instruct<br></code></pre></td></tr></table></figure><h2 id="模型推理-——-基于-transformers"><a href="#模型推理-——-基于-transformers" class="headerlink" title="模型推理 —— 基于 transformers"></a>模型推理 —— 基于 transformers</h2><p>安装依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install git+https://github.com/huggingface/transformers<br></code></pre></td></tr></table></figure><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install qwen-vl-utils<br></code></pre></td></tr></table></figure><h3 id="单图推理"><a href="#单图推理" class="headerlink" title="单图推理"></a>单图推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image<br><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> Qwen2VLForConditionalGeneration, AutoTokenizer, AutoProcessor<br><span class="hljs-keyword">from</span> qwen_vl_utils <span class="hljs-keyword">import</span> process_vision_info<br><span class="hljs-keyword">from</span> modelscope <span class="hljs-keyword">import</span> snapshot_download<br><br><br>model_dir = <span class="hljs-string">&quot;/mnt/workspace/Qwen2-VL-2B-Instruct&quot;</span><br>min_pixels = <span class="hljs-number">256</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br>max_pixels = <span class="hljs-number">1280</span>*<span class="hljs-number">28</span>*<span class="hljs-number">28</span><br><br><span class="hljs-comment"># Load the model in half-precision on the available device(s)</span><br>model = Qwen2VLForConditionalGeneration.from_pretrained(<br>    model_dir, device_map=<span class="hljs-string">&quot;auto&quot;</span>, torch_dtype = torch.float16<br>)<br><span class="hljs-comment"># Load the processor</span><br>processor = AutoProcessor.from_pretrained(<br>model_dir, min_pixels=min_pixels, max_pixels=max_pixels<br>)<br><br>messages = [<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <br>        <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-string">&quot;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&quot;</span>&#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Describe this image.&quot;</span>&#125;<br>        ]<br>    &#125;<br>]<br><br><span class="hljs-comment"># Preparation for inference</span><br>text = processor.apply_chat_template(<br>    messages, tokenize=<span class="hljs-literal">False</span>, add_generation_prompt=<span class="hljs-literal">True</span><br>)<br>image_inputs, video_inputs = process_vision_info(messages)<br>inputs = processor(<br>text=[text], images=image_inputs, videos=video_inputs, <br>    padding=<span class="hljs-literal">True</span>, return_tensors=<span class="hljs-string">&quot;pt&quot;</span><br>)<br>inputs = inputs.to(<span class="hljs-string">&#x27;cuda&#x27;</span>)<br><br><span class="hljs-comment"># Inference: Generation of the output</span><br>generated_ids = model.generate(**inputs, max_new_tokens=<span class="hljs-number">128</span>)<br>generated_ids_trimmed = [<br>    out_ids[<span class="hljs-built_in">len</span>(in_ids):] <span class="hljs-keyword">for</span> in_ids, out_ids <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(inputs.input_ids, generated_ids)<br>]<br><br>output_text = processor.batch_decode(<br>generated_ids_trimmed, skip_special_tokens=<span class="hljs-literal">True</span>, clean_up_tokenization_spaces=<span class="hljs-literal">False</span><br>)<br><br><span class="hljs-built_in">print</span>(output_text)<br></code></pre></td></tr></table></figure><h3 id="多图推理"><a href="#多图推理" class="headerlink" title="多图推理"></a>多图推理</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Messages containing multiple images and a text query</span><br>messages = [<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <br>        <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-string">&quot;file:///path/to/image1.jpg&quot;</span>&#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image&quot;</span>, <span class="hljs-string">&quot;image&quot;</span>: <span class="hljs-string">&quot;file:///path/to/image2.jpg&quot;</span>&#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Identify the similarities between these images.&quot;</span>&#125;<br>        ]<br>    &#125;<br>]<br></code></pre></td></tr></table></figure><h3 id="视频理解"><a href="#视频理解" class="headerlink" title="视频理解"></a>视频理解</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># Messages containing a video and a text query</span><br>messages = [<br>    &#123;<br>        <span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <br>        <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<br>                <span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;video&quot;</span>, <br>                <span class="hljs-string">&quot;video&quot;</span>: <span class="hljs-string">&quot;file:///path/to/video1.mp4&quot;</span>, <br>                <span class="hljs-string">&#x27;max_pixels&#x27;</span>: <span class="hljs-number">360</span>*<span class="hljs-number">420</span>, <br>                <span class="hljs-string">&#x27;fps&#x27;</span>: <span class="hljs-number">1.0</span><br>            &#125;, <br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;Describe this video.&quot;</span>&#125;<br>        ]<br>    &#125;<br>]<br></code></pre></td></tr></table></figure><h2 id="模型推理-——-基于-vllm"><a href="#模型推理-——-基于-vllm" class="headerlink" title="模型推理 —— 基于 vllm"></a>模型推理 —— 基于 vllm</h2><p>安装依赖</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install git+https://github.com/fyabc/vllm.git@add_qwen2_vl_new<br></code></pre></td></tr></table></figure><p>启动 OpenAI 接口服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">python -m vllm.entrypoints.openai.api_server --served-model-name Qwen2-VL-7B-Instruct --model model_path<br></code></pre></td></tr></table></figure><p>调用服务</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl http://localhost:8000/v1/chat/completions \    <br>-H <span class="hljs-string">&quot;Content-Type: application/json&quot;</span> \    <br>-d <span class="hljs-string">&#x27;&#123;    </span><br><span class="hljs-string">&quot;model&quot;: &quot;Qwen2-VL-7B-Instruct&quot;,    </span><br><span class="hljs-string">&quot;messages&quot;: [    </span><br><span class="hljs-string">&#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are a helpful assistant.&quot;&#125;,    </span><br><span class="hljs-string">&#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: [        </span><br><span class="hljs-string">&#123;&quot;type&quot;: &quot;image_url&quot;, &quot;image_url&quot;: &#123;&quot;url&quot;: &quot;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&quot;&#125;&#125;,        </span><br><span class="hljs-string">&#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: &quot;What is the text in the illustrate?&quot;&#125;    </span><br><span class="hljs-string">]&#125;    </span><br><span class="hljs-string">]    </span><br><span class="hljs-string">&#125;&#x27;</span><br></code></pre></td></tr></table></figure><p>调用服务</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> openai <span class="hljs-keyword">import</span> OpenAI<br><br><span class="hljs-comment"># Set OpenAI&#x27;s API key and API base to use vLLM&#x27;s API server.</span><br>openai_api_key = <span class="hljs-string">&quot;EMPTY&quot;</span><br>openai_api_base = <span class="hljs-string">&quot;http://localhost:8000/v1&quot;</span><br><br>client = OpenAI(<br>api_key=openai_api_key, base_url=openai_api_base<br>)<br><br>chat_response = client.chat.completions.create(<br>model=<span class="hljs-string">&quot;Qwen2-7B-Instruct&quot;</span>,    <br>    messages=[<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;system&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: <span class="hljs-string">&quot;You are a helpful assistant.&quot;</span>&#125;,<br>        &#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;user&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: [<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;image_url&quot;</span>, <span class="hljs-string">&quot;image_url&quot;</span>: &#123;<span class="hljs-string">&quot;url&quot;</span>: <span class="hljs-string">&quot;https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png&quot;</span>&#125;&#125;,<br>            &#123;<span class="hljs-string">&quot;type&quot;</span>: <span class="hljs-string">&quot;text&quot;</span>, <span class="hljs-string">&quot;text&quot;</span>: <span class="hljs-string">&quot;What is the text in the illustrate?&quot;</span>&#125;,        <br>        ]&#125;,<br>]<br>)<br><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Chat response:&quot;</span>, chat_response)<br></code></pre></td></tr></table></figure><h2 id="模型微调-——-基于-swift"><a href="#模型微调-——-基于-swift" class="headerlink" title="模型微调 —— 基于 swift"></a>模型微调 —— 基于 swift</h2><p>在开始微调之前，请确保您的环境已准备妥当。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">git <span class="hljs-built_in">clone</span> https://github.com/modelscope/swift.gitcd swift<br></code></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> -e .[llm]<br></code></pre></td></tr></table></figure><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs cmake">pip <span class="hljs-keyword">install</span> pyav qwen_vl_utils<br></code></pre></td></tr></table></figure><h3 id="图像描述微调"><a href="#图像描述微调" class="headerlink" title="图像描述微调"></a>图像描述微调</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 默认会将 lora_target_modules 设置为 llm 的所有linear</span><br>CUDA_VISIBLE_DEVICES=<span class="hljs-number">0</span>,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span> NPROC_PER_NODE=<span class="hljs-number">4</span> swift sft \  <br>--model_type qwen2-vl-7b-instruct \  <br>    --model_id_or_path qwen/Qwen2-VL-7B-Instruct \  <br>    --sft_type lora \  <br>    --dataset coco-en-mini<span class="hljs-comment">#20000 \  </span><br>    --deepspeed default-zero2<br>    --dataset train.jsonl \  <br>    --val_dataset val.jsonl \  <span class="hljs-comment"># 自定义数据集</span><br></code></pre></td></tr></table></figure><p>以下是自定义数据集的样例：</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;image&gt;55555&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;66666&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;image_path&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;eeeee&lt;image&gt;eeeee&lt;image&gt;eeeee&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;fffff&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;image_path1&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;image_path2&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;EEEEE&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;FFFFF&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;history&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">[</span><span class="hljs-string">&quot;query1&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;response2&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;query2&quot;</span><span class="hljs-punctuation">,</span> <span class="hljs-string">&quot;response2&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><p>微调后推理脚本如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">CUDA_VISIBLE_DEVICES=0 swift infer \    <br>--ckpt_dir output/qwen2-vl-7b-instruct/vx-xxx/checkpoint-xxx \    <br>--load_dataset_config <span class="hljs-literal">true</span> <br>--merge_lora <span class="hljs-literal">true</span><br></code></pre></td></tr></table></figure><h3 id="图像-grounding-微调"><a href="#图像-grounding-微调" class="headerlink" title="图像 grounding 微调"></a>图像 grounding 微调</h3><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs json"># swift 跨模型通用格式<br><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Find &lt;bbox&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;ref-object&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;/coco2014/train2014/COCO_train2014_000000001507.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;objects&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;[&#123;\&quot;caption\&quot;: \&quot;guy in red\&quot;, \&quot;bbox\&quot;: [138, 136, 235, 359], \&quot;bbox_type\&quot;: \&quot;real\&quot;, \&quot;image\&quot;: 0&#125;]&quot;</span> <br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span><br>    <span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Find &lt;ref-object&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;bbox&gt;&quot;</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;/coco2014/train2014/COCO_train2014_000000001507.jpg&quot;</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> <br>    <span class="hljs-attr">&quot;objects&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;[&#123;\&quot;caption\&quot;: \&quot;guy in red\&quot;, \&quot;bbox\&quot;: [138, 136, 235, 359], \&quot;bbox_type\&quot;: \&quot;real\&quot;, \&quot;image\&quot;: 0&#125;]&quot;</span> <br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs json"># qwen2-vl-chat 特定格式，注意特殊字符的存在<br><span class="hljs-punctuation">&#123;</span><br><span class="hljs-attr">&quot;query&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;Find &lt;|object_ref_start|&gt;the man&lt;|object_ref_end|&gt;&quot;</span><span class="hljs-punctuation">,</span> <br><span class="hljs-attr">&quot;response&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-string">&quot;&lt;|box_start|&gt;(123,235),(324,546)&lt;|box_end|&gt;&quot;</span><span class="hljs-punctuation">,</span> <br><span class="hljs-attr">&quot;images&quot;</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-string">&quot;/coco2014/train2014/COCO_train2014_000000001507.jpg&quot;</span><span class="hljs-punctuation">]</span><br><span class="hljs-punctuation">&#125;</span><br></code></pre></td></tr></table></figure><h3 id="视频微调"><a href="#视频微调" class="headerlink" title="视频微调"></a>视频微调</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs bash">NFRAMES=24 MAX_PIXELS=100352 CUDA_VISIBLE_DEVICES=0,1,2,3 NPROC_PER_NODE=4 swift sft \  <br>--model_type qwen2-vl-7b-instruct \  <br>--model_id_or_path qwen/Qwen2-VL-7B-Instruct \  <br>--sft_type lora \  <br>--dataset video-chatgpt \  <br>--deepspeed default-zero2<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记-第2篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC2%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第二代-Qwen2-VL-2024-09-2024-10"><a href="#第二代-Qwen2-VL-2024-09-2024-10" class="headerlink" title="第二代 Qwen2-VL 2024.09-2024.10"></a>第二代 Qwen2-VL 2024.09-2024.10</h1><blockquote><p>参考网页：</p><p><a href="https://zhuanlan.zhihu.com/p/7352653203">【多模态大模型】Qwen2-VL解剖</a></p><p><a href="https://zhuanlan.zhihu.com/p/719388479">Qwen2-VL技术解析（二）- M-ROPE</a></p></blockquote><ul><li>引入<b>原生动态分辨率（Naive Dynamic Resolution, NDR）</b>机制 —— 实现了对<u>任意分辨率图像&#x2F;视频</u>的灵活处理。</li><li>设计<b>多模态旋转位置嵌入（Multimodal Rotary Position Embedding, M-RoPE）</b> —— 提升了模型的跨模态理解能力，还增强了长序列任务中的推理表现，在视频内容理解中尤为突出。</li><li>更强的复杂推理和决策的能力，可根据视觉环境和文字指令进行自动操作手机、机器人等设备。</li><li>支持除英语和中文外，也支持大多数欧洲语言、日语、韩语、阿拉伯语、越南语等<b>多语言</b>。</li><li>统一的多模态学习框架 —— 确保模型对多模态任务的全面适配。</li><li>显著性能提升 —— 在多个权威基准数据集上实现了新 SoTA 。</li></ul><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p><img src="/_posts/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/qwen2_vl_frame.jpg"></p><ul><li><p>视觉编码器</p><p>图像和视频帧经过基于 ViT 的视觉编码器后，生成一系列视觉特征 token 。取代传统绝对位置编码，模型通过二维 RoPE 嵌入捕捉图像中<b>像素空间分布</b>信息。为了降低计算复杂度，视觉特征通过<u>一个简单的 MLP 层</u>进一步压缩，将邻近的 2x2 视觉 token 合并为一个 token ，同时在 token 序列的起始和结尾处分别添加<code>&lt;|vision_start|&gt;</code> 和 <code>&lt;|vision_end|&gt;</code> 标记。</p></li><li><p>语言模型</p><p>使用 Qwen2-LM ，Qwen2 系列语言模型都是基于 Transformer 的解码器结构。视觉特征 tokens 和文本 tokens 通过统一的输入序列被送入 Qwen2 系列语言模型中，进行多模态信息的融合和处理。</p></li><li><p>多模态旋转位置嵌入（M-RoPE）</p><blockquote><p>参考网页：<a href="https://zhuanlan.zhihu.com/p/719388479">https://zhuanlan.zhihu.com/p/719388479</a></p></blockquote><p>M-RoPE 在视觉和文本 token 之间建立精确的位置信息关联。特别是在视频任务中， M-RoPE 能够捕捉时间维度的动态变化，使模型能够处理长时间的视频内容。</p><p>对于文本，位置编码与传统的 1D-RoPE 一致，采用<u>一维序列编号</u>来标记词位位置。</p><p>对于图像，M-RoPE 通过<u>二维位置编码</u>为每个视觉 token 分配宽度和高度的位置标识，以准确表示图像中的空间结构。同时，图像中的时间维度编码值保持固定，因为静态图像没有时间变化。</p><p>对于视频，M-RoPE 进一步扩展，通过对<b>每一帧引入递增的时间位置标识</b>，并结合图像的宽度和高度位置编码，使模型能够理解视频帧的动态时间序列信息。</p></li></ul><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：模型根据输入图像的分辨率，动态调整视觉 token 的数量，而不是将所有图像固定调整到同一尺寸。</p><p>视频输入：对于视频数据，为了一致性，每个图像被视为两个相同的帧。为了平衡长视频处理的计算需求和整体训练效率，动态调整每个视频帧的分辨率，将每个视频的令牌总数限制在 16384 个，并利用动态分辨率机制压缩多帧内容为适合语言模型处理的视觉 token 序列。</p><p>模型输出：</p><ul><li>视觉问答（VQA）：回答与图像或视频相关的问题。</li><li>文档理解与 OCR ：识别和解析复杂文档中的文本信息。</li><li>视频内容分析：对长视频内容进行总结、生成对话内容或回答基于视频的问题。</li><li>代理能力：支持设备操作，如根据屏幕截图导航手机界面。</li></ul><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：ViT-训练，冻结语言模型参数"><a href="#第一阶段：ViT-训练，冻结语言模型参数" class="headerlink" title="第一阶段：ViT 训练，冻结语言模型参数"></a>第一阶段：ViT 训练，冻结语言模型参数</h3><p>使用大量的 “图像—文本” 对数据集来增强大语言模型（LLM）中的语义理解，让模型学会图像和文本之间的关系，以及通过 OCR 在图像中识别文本内容和图像分类任务。</p><p>在这一阶段，Qwen2-VL 大约使用 6000 亿个标记的语料库预训练。LLM 部分使用 Qwen2 的参数进行初始化，而 ViT 部分则使用<u>从 DFN 派生的 ViT</u> 进行初始化。</p><blockquote><p>DFN 即 <strong>Data Filtering Network</strong>，是一个<strong>小型的、专门用于 “筛数据” 的神经网络</strong>。它通常会接收<strong>图像—文本对</strong>，并预测其质量评分，高分样本被保留用于训练大型预训练模型。它本身不直接生成训练模型，而是在 “构建训练集” 的中间环节发挥作用。</p></blockquote><h3 id="第二阶段：解冻-ViT-和语言模型进行全参数训练"><a href="#第二阶段：解冻-ViT-和语言模型进行全参数训练" class="headerlink" title="第二阶段：解冻 ViT 和语言模型进行全参数训练"></a>第二阶段：解冻 ViT 和语言模型进行全参数训练</h3><p>解冻所有参数，并使用更广泛的数据进行训练，以实现更全面的学习。这个阶段引入了额外的 8000 亿个图像相关数据的标记，通过引入更多的混合 “图像—文本” 内容，以促进视觉和文本信息之间更细微的理解。</p><h3 id="第三阶段：LLM-的微调"><a href="#第三阶段：LLM-的微调" class="headerlink" title="第三阶段：LLM 的微调"></a>第三阶段：LLM 的微调</h3><p>使用 ChatML 格式构建指令跟随数据。冻结 ViT 参数，使用指令数据集对 LLM 进行专门的微调。</p>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>QwenVL系列多模态模型学习笔记_第1篇</title>
    <link href="/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/"/>
    <url>/2025/06/25/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/</url>
    
    <content type="html"><![CDATA[<h1 id="第一代-Qwen-VL-2023-08-2023-10"><a href="#第一代-Qwen-VL-2023-08-2023-10" class="headerlink" title="第一代 Qwen-VL 2023.08-2023.10"></a>第一代 Qwen-VL 2023.08-2023.10</h1><blockquote><p>参考：</p><p><a href="https://zhuanlan.zhihu.com/p/689914137">Qwen-VL看这一篇就够了</a></p></blockquote><p>当时大多数的 LVLMs 都是以<b>粗粒度</b>的方式感知图像，缺乏<u>图像细粒度感知</u>的能力（包括<b>目标定位</b>和<b>文本读取</b>等）。基于当时的问题，Qwen 团队引入了一个<u>新的视觉编码器</u>和<u>位置感知适配器</u>，并且设计了一个三阶段训练的流程用于优化 Qwen-VL 模型。Qwen-VL 的特点：性能领先、支持多语言、<strong>支持任意交错的 “图像-文本” 数据</strong>、<strong>细粒度的视觉理解</strong>（例如 OCR）。Qwen-VL 相较于之前的<b>图文多模态大模型</b>多了一个功能：视觉定位，就是可以<b>给出一个框</b>将你想要的地方框出来。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p><img src="/_posts/QwenVL%E7%B3%BB%E5%88%97%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0-%E7%AC%AC1%E7%AF%87/qwenvl_01.png"></p><blockquote><p>通常一个多模态 “视觉—语言” 模型包含三个结构：语言模型、视觉编码器和 “视觉—语言” 适配器。</p></blockquote><p>Qwen-VL 整个模型参数大致在 1.9B + 0.08B + 7.7B &#x3D; 9.6B 的参数数量。</p><ol><li><p>语言模型：Qwen-7B 大语言模型；</p></li><li><p>视觉编码器：ViT 的架构，参数量在 1.9B ，并且从<a href="https://github.com/mlfoundations/open_clip">开源项目 openclip 的 ViT-bigG</a> 权重开始初始化，训练和推理的过程中图像会被调整到特定的分辨率，也就是拆成 14x14 像素的 patch 块；</p></li><li><p>（位置级）视觉语言适配器：一个<u>随机权重初始化</u>的<u>单层交叉注意力模块</u>组成，参数量在 0.08B 。</p><p>该模块使用一组<b>可训练的向量（意思就是在训练中张量数值会改变，且梯度会流向这个向量）</b>作为 query 向量，将<b>视觉编码器的特征</b>作为 key 进行交叉注意力操作，将图像特征压缩到 256 长度的序列。并且将 2D 绝对位置编码用在交叉注意力机制中，以减轻压缩过程中的位置细节丢失。</p></li></ol><h2 id="模型输入和输出"><a href="#模型输入和输出" class="headerlink" title="模型输入和输出"></a>模型输入和输出</h2><p>图像输入：<code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code> 标记图像的开始和结束。图片通过<u>视觉编码器</u>和<u>（位置级）视觉语言适配器</u>模块，得到一个定长的特征序列。为了和文字输入区别，图片特征前后分别加上 <code>&lt;img&gt;</code> 和 <code>&lt;/img&gt;</code>。</p><p>边界框输出：将边界框的值归一化在 <code>[0,1000)</code> 之间，并转换成特定的字符串格式 <code>&quot;(X_top_left, Y_top_left), (X_bottom_right, Y_bottom_right)&quot;</code> ，<code>&lt;box&gt;</code> 和 <code>&lt;/box&gt;</code> 分别添加在边界框字符串的开头和结尾。</p><p>内容输出：<code>&lt;ref&gt;</code> 和 <code>&lt;/ref&gt;</code> 标记边界框所引用的内容。</p><blockquote><p>例如，某个任务的提示词：</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs css">&lt;<span class="hljs-selector-tag">img</span>&gt;coyo700m/<span class="hljs-number">1</span><span class="hljs-selector-class">.jpg</span>&lt;/<span class="hljs-selector-tag">img</span>&gt;Generate the <span class="hljs-selector-tag">caption</span> in English with grounding:<br></code></pre></td></tr></table></figure><p>Qwen-VL 的回答如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs apache"><span class="hljs-attribute">Beautiful</span> shot of &lt;ref&gt;bees&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">661</span>,<span class="hljs-number">612</span>),(<span class="hljs-number">833</span>,<span class="hljs-number">812</span>)&lt;/box&gt;&lt;box&gt;(<span class="hljs-number">120</span>,<span class="hljs-number">555</span>),(<span class="hljs-number">265</span>,<span class="hljs-number">770</span>)&lt;/box&gt; gathering nectars from &lt;ref&gt;an apricot flower&lt;/ref&gt;&lt;box&gt;(<span class="hljs-number">224</span>,<span class="hljs-number">13</span>),(<span class="hljs-number">399</span>,<span class="hljs-number">313</span>) &lt;/box&gt;&lt;eos&gt;<br></code></pre></td></tr></table></figure></blockquote><p>模型处理视觉信息的代码如下所示：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 确保只在第一次 forward 时处理视觉信息（past_key_values is None 表示不是缓存推理时）</span><br><span class="hljs-comment"># 图像 token 是以特殊的 image_start_id 开始，检测是否存在</span><br><span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">and</span> torch.<span class="hljs-built_in">any</span>(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>]):<br>    <br>    <span class="hljs-comment"># 找到图像 token 的边界位置</span><br>        bos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>])<br>        eos_pos = torch.where(input_ids == <span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">1</span>)<br>        <br>        <span class="hljs-comment"># 保证每个起始标记都在一个样本内部结束</span><br>        <span class="hljs-keyword">assert</span> (bos_pos[<span class="hljs-number">0</span>] == eos_pos[<span class="hljs-number">0</span>]).<span class="hljs-built_in">all</span>()<br>        <br>        <span class="hljs-comment"># 构建 img_pos：(batch_idx, start_idx, end_idx)</span><br>        img_pos = torch.stack((bos_pos[<span class="hljs-number">0</span>], bos_pos[<span class="hljs-number">1</span>], eos_pos[<span class="hljs-number">1</span>]), dim=<span class="hljs-number">1</span>)<br>        images = []<br>        <span class="hljs-keyword">for</span> i, a, b <span class="hljs-keyword">in</span> img_pos:<br>            <br>            <span class="hljs-comment"># 截取 patch token（跳过起始和终止标志）</span><br>            image = input_ids[i][a + <span class="hljs-number">1</span> : b - <span class="hljs-number">1</span>].tolist()<br>            <span class="hljs-comment"># 截取图片结束 token</span><br>            image = image[ : image.index(<span class="hljs-variable language_">self</span>.config.visual[<span class="hljs-string">&#x27;image_start_id&#x27;</span>] + <span class="hljs-number">2</span>)]<br>            <span class="hljs-comment"># 解码为 utf-8 图像数据（说明 image token 实际是原始图片字节的编码）</span><br>            images.append(<span class="hljs-built_in">bytes</span>(image).decode(<span class="hljs-string">&#x27;utf-8&#x27;</span>))<br>            <br>            <span class="hljs-comment"># 调用视觉编码器</span><br>            images = <span class="hljs-variable language_">self</span>.visual.encode(images)<br>            <span class="hljs-keyword">assert</span> images.shape[<span class="hljs-number">0</span>] == <span class="hljs-built_in">len</span>(images)<br>            <br>            fake_images = <span class="hljs-literal">None</span><br>            <span class="hljs-keyword">if</span> fake_images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                hidden_states = hidden_states + images.mean()*<span class="hljs-number">0</span><br>            <br><span class="hljs-comment"># 将图像嵌入写入对应位置 a+1 : b 的 hidden state（对应 patch tokens）</span><br>            <span class="hljs-keyword">elif</span> images <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                <span class="hljs-keyword">for</span> idx, (i, a, b) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(img_pos):<br>                    hidden_states[i][a + <span class="hljs-number">1</span> : b] = images[idx]<br></code></pre></td></tr></table></figure><blockquote><p>Q：<code>image = image[ : image.index(self.config.visual[&#39;image_start_id&#39;] + 2)]</code> 这段代码的作用？</p><p>A：因为模型在输入图片时，有时会预留更多 token 空间来填充图像信息，例如 padding 和 filter ，这就导致图像本身可能变长。这些填充的 token 确实是图像的一部分，但是输入到视觉编码器中产生干扰，因此需要额外再加一行代码对这些填充 token 做进一步过滤。</p></blockquote><h2 id="模型训练过程"><a href="#模型训练过程" class="headerlink" title="模型训练过程"></a>模型训练过程</h2><h3 id="第一阶段：预训练过程"><a href="#第一阶段：预训练过程" class="headerlink" title="第一阶段：预训练过程"></a>第一阶段：预训练过程</h3><p>使用互联网网页抓取的 ”图像—文本“ 对，50 亿条数据清洗后剩下 14 亿数据，其中 77.3% 为英文数据，22.7% 为中文数据。<b>这一阶段冻结语言模型</b>，训练视觉编码器和视觉语言适配器，输入图像调整为 224x224 的分辨率（按照每 14 像素分割后得到 16x16&#x3D;256 个 patch），batch size 为 30720 ，训练 50000 步，使用 15 亿数据。</p><h3 id="第二阶段：多任务预训练"><a href="#第二阶段：多任务预训练" class="headerlink" title="第二阶段：多任务预训练"></a>第二阶段：多任务预训练</h3><p>加入了高质量、细粒度的图像和文本数据，使用了更大的分辨率和交错的 ”图像—文本“ 数据。在 7 个任务上对 Qwen-VL 进行训练。将视觉编码器的分辨率从 224x224 增加到 448x448，以减少图像下采样造成的信息损失。<b>这一过程没有冻结任何模块。</b></p><h3 id="第三阶段：监督微调"><a href="#第三阶段：监督微调" class="headerlink" title="第三阶段：监督微调"></a>第三阶段：监督微调</h3><p>数据<u>来自 LLM 生成</u>的图像标注或对话数据，这些数据通常只处理<b>单图像对话和推理</b>，且仅限于图像内容理解。</p><p>通过<u>手动标注、模型生成和策略组合</u>，构建了一个额外的对话数据集，以将<b>定位和多图像理解能力</b>融入 Qwen-VL 模型中。在训练过程中混合了多模态和<u>纯文本对话数据</u>，以确保模型的对话能力具有普遍性。</p><p>指令微调数据量达到 35 万条。<b>这一过程冻结视觉编码器。</b></p><h2 id="模型代码应用"><a href="#模型代码应用" class="headerlink" title="模型代码应用"></a>模型代码应用</h2><h3 id="图片和文本的加载"><a href="#图片和文本的加载" class="headerlink" title="图片和文本的加载"></a>图片和文本的加载</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">query = tokenizer.from_list_format([<br>    &#123;<span class="hljs-string">&#x27;image&#x27;</span>: <span class="hljs-string">&#x27;https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg&#x27;</span>&#125;,<br>    &#123;<span class="hljs-string">&#x27;text&#x27;</span>: <span class="hljs-string">&#x27;这是什么&#x27;</span>&#125;,<br>])<br></code></pre></td></tr></table></figure><h3 id="图像到字符串的转换"><a href="#图像到字符串的转换" class="headerlink" title="图像到字符串的转换"></a>图像到字符串的转换</h3><p>Qwen-VL 将图片都处理成：“Picture 1”、“Picture 2”、“Picture 3” 等<u>字符串</u>格式，并添加上<u>图片的开始和结束 token</u> ，文本直接拼接，box 的 <u>ref 添加上开始结束符</u>拼接，box <u>坐标从数字整理成字符串格式</u>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><code class="hljs PYTHON"><span class="hljs-keyword">def</span> <span class="hljs-title function_">from_list_format</span>(<span class="hljs-params">self, list_format: <span class="hljs-type">List</span>[<span class="hljs-type">Dict</span>]</span>):<br>    text = <span class="hljs-string">&#x27;&#x27;</span><br>    num_images = <span class="hljs-number">0</span><br>    <span class="hljs-keyword">for</span> ele <span class="hljs-keyword">in</span> list_format:  <span class="hljs-comment"># 每个 ele 都是字典</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;image&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 图片处理成这样的字符串，再加上图像自身、开始和结束的 tokens</span><br>            num_images += <span class="hljs-number">1</span><br>            text += <span class="hljs-string">f&#x27;Picture <span class="hljs-subst">&#123;num_images&#125;</span>: &#x27;</span><br>            text += <span class="hljs-variable language_">self</span>.image_start_tag + ele[<span class="hljs-string">&#x27;image&#x27;</span>] + <span class="hljs-variable language_">self</span>.image_end_tag<br>            text += <span class="hljs-string">&#x27;\n&#x27;</span><br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;text&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是文本，直接添加文本</span><br>            text += ele[<span class="hljs-string">&#x27;text&#x27;</span>]<br>            <br>        <span class="hljs-keyword">elif</span> <span class="hljs-string">&#x27;box&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>            <span class="hljs-comment"># 如果是定位框，先考虑有没有参考对象</span><br>            <span class="hljs-comment"># 如果有的话，先添加参考对象自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-comment"># 没有的话，添加定位框自身字符串、开始和结束的 tokens</span><br>            <span class="hljs-keyword">if</span> <span class="hljs-string">&#x27;ref&#x27;</span> <span class="hljs-keyword">in</span> ele:<br>                text += <span class="hljs-variable language_">self</span>.ref_start_tag + ele[<span class="hljs-string">&#x27;ref&#x27;</span>] + <span class="hljs-variable language_">self</span>.ref_end_tag<br>            <span class="hljs-keyword">for</span> box <span class="hljs-keyword">in</span> ele[<span class="hljs-string">&#x27;box&#x27;</span>]:<br>                text += <span class="hljs-variable language_">self</span>.box_start_tag + <span class="hljs-string">&#x27;(%d,%d),(%d,%d)&#x27;</span> % (box[<span class="hljs-number">0</span>], box[<span class="hljs-number">1</span>], box[<span class="hljs-number">2</span>], box[<span class="hljs-number">3</span>]) + <span class="hljs-variable language_">self</span>.box_end_tag<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Unsupport element: &quot;</span> + <span class="hljs-built_in">str</span>(ele))<br>    <span class="hljs-keyword">return</span> text<br></code></pre></td></tr></table></figure><h3 id="图像的编码"><a href="#图像的编码" class="headerlink" title="图像的编码"></a>图像的编码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">encode</span>(<span class="hljs-params">self, image_paths: <span class="hljs-type">List</span>[<span class="hljs-built_in">str</span>]</span>):<br>        images = []<br>        <span class="hljs-keyword">for</span> image_path <span class="hljs-keyword">in</span> image_paths:<br>            <span class="hljs-keyword">if</span> image_path.startswith(<span class="hljs-string">&quot;http://&quot;</span>) <span class="hljs-keyword">or</span> image_path.startswith(<span class="hljs-string">&quot;https://&quot;</span>):<br>                image = Image.<span class="hljs-built_in">open</span>(requests.get(image_path, stream=<span class="hljs-literal">True</span>).raw)<br>            <span class="hljs-keyword">else</span>:<br>                image = Image.<span class="hljs-built_in">open</span>(image_path)<br>            image = image.convert(<span class="hljs-string">&quot;RGB&quot;</span>)<br>            images.append(<span class="hljs-variable language_">self</span>.image_transform(image))<br>        images = torch.stack(images, dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>(images)<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x: torch.Tensor</span>):<br>        x = x.to(<br>            dtype=<span class="hljs-variable language_">self</span>.transformer.get_cast_dtype(),<br>            device=<span class="hljs-variable language_">self</span>.transformer.get_cast_device(),<br>        )<br>        <span class="hljs-comment"># to patches</span><br>        x = <span class="hljs-variable language_">self</span>.conv1(x)  <span class="hljs-comment"># shape = [*, width, grid, grid]</span><br>        x = x.reshape(x.shape[<span class="hljs-number">0</span>], x.shape[<span class="hljs-number">1</span>], -<span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, width, grid ** 2]</span><br>        x = x.permute(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)  <span class="hljs-comment"># shape = [*, grid ** 2, width]</span><br><br>        x = x + get_abs_pos(<span class="hljs-variable language_">self</span>.positional_embedding, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.ln_pre(x)<br><br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># NLD -&gt; LND</span><br>        x = <span class="hljs-variable language_">self</span>.transformer(x)<br>        x = x.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)  <span class="hljs-comment"># LND -&gt; NLD</span><br><br>        x = <span class="hljs-variable language_">self</span>.attn_pool(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_post(x)<br>        x = x @ <span class="hljs-variable language_">self</span>.proj<br></code></pre></td></tr></table></figure><p>图片是经过 resize 和归一化后输入 ViT 进行编码，ViT 编码后经过交叉注意力机制、归一化然后投影到 embedding 维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-variable language_">self</span>.attn_pool = Resampler(<br>            grid_size=<span class="hljs-built_in">int</span>(math.sqrt(n_queries)),<br>            embed_dim=output_dim,<br>            num_heads=output_dim // <span class="hljs-number">128</span>,<br>            kv_dim=width,<br>            norm_layer=norm_layer,<br>        )<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, attn_mask=<span class="hljs-literal">None</span></span>):<br><br>        pos_embed = get_abs_pos(<span class="hljs-variable language_">self</span>.pos_embed, x.size(<span class="hljs-number">1</span>))<br><br>        x = <span class="hljs-variable language_">self</span>.kv_proj(x)<br>        x = <span class="hljs-variable language_">self</span>.ln_kv(x).permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br><br>        N = x.shape[<span class="hljs-number">1</span>]<br>        q = <span class="hljs-variable language_">self</span>.ln_q(<span class="hljs-variable language_">self</span>.query)<br>        out = <span class="hljs-variable language_">self</span>.attn(<br>            <span class="hljs-variable language_">self</span>._repeat(q, N) + <span class="hljs-variable language_">self</span>.pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x + pos_embed.unsqueeze(<span class="hljs-number">1</span>),<br>            x,<br>            attn_mask=attn_mask)[<span class="hljs-number">0</span>]<br>        <span class="hljs-keyword">return</span> out.permute(<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>多模态大模型</tag>
      
      <tag>Qwen-VL</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
